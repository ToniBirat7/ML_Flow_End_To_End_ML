{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2eae0d8b",
   "metadata": {},
   "source": [
    "## **Working of with Keyowrd**\n",
    "\n",
    "[GPT_Explanation](https://chatgpt.com/share/681f4657-6520-8006-b695-a215a1783899)\n",
    "\n",
    "[Real_Python_Implementation](https://www.youtube.com/watch?v=iba-I4CrmyA)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3429f5bc",
   "metadata": {},
   "source": [
    "## **Before Starting Project**\n",
    "\n",
    "`source mlflow_env/bin/activate`\n",
    "\n",
    "`mlflow ui`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a79da5f",
   "metadata": {},
   "source": [
    "## **ML Flow**\n",
    "\n",
    "Open-source. We can track our Machine Learning project such as performance metrices etc.\n",
    "\n",
    "## **Lifecycle of a Data Science Project**\n",
    "\n",
    "**Data Pre**\n",
    "\n",
    "**EDA**\n",
    "\n",
    "**Feature Eng**\n",
    "\n",
    "**Model Training**\n",
    "\n",
    "**Model Validation**\n",
    "\n",
    "**Deployment**\n",
    "\n",
    "**Monitoring**\n",
    "\n",
    "## **How ML Flow is used by Data Scientist**\n",
    "\n",
    "- Experiment Tracking\n",
    "\n",
    "- Hypothesis Testing in EDA\n",
    "\n",
    "- Code Structuring (Pipeline)\n",
    "\n",
    "- Model Packaging and Dependency Management\n",
    "\n",
    "- Evaluating Hyperparameter : Track every combination of Hyperparameter\n",
    "\n",
    "- Compare the results of model and deploy the best performing model\n",
    "\n",
    "## **How ML Flow is used by ML Engineeer**\n",
    "\n",
    "- Manage the lifecycle of trained models both pre and post deployment\n",
    "\n",
    "- Deploy models security to the production env\n",
    "\n",
    "- Manage Deployment Dependencies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d72f24",
   "metadata": {},
   "source": [
    "## **ML Flow Starter**\n",
    "\n",
    "### **ML Flow Tracking Server**\n",
    "\n",
    "For tracking our experiments we need to create a server\n",
    "\n",
    "To start the server we use `mlflow ui`.\n",
    "\n",
    "Then we will need to provide the tracking UI so that everything is tracked by MLFlow. `mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")`\n",
    "\n",
    "Then to log our performance metrices we will use as below:\n",
    "\n",
    "```py\n",
    "\n",
    "mlflow.set_experiment(\"Day2\")\n",
    "\n",
    "# Start the MLFlow Run\n",
    "\n",
    "with mlflow.start_run():\n",
    "  # Log the hyperparameters\n",
    "  mlflow.log_params(params)\n",
    "\n",
    "\t# Log te accuracy metrics\n",
    "  mlflow.log_metric(\"Accuracy\",accuracy)\n",
    "\n",
    "\t# Set tag that we can use to remind ourselves what this run was for\n",
    "  mlflow.set_tag(\"Training Info\", \"Basic LR Model for Iris Data\")\n",
    "\n",
    "\t# Infer the model signature\n",
    "  signature = infer_signature(x_train, model.predict(x_train))\n",
    "\n",
    "\t# Log the model\n",
    "  model_info = mlflow.sklearn.log_model(\n",
    "    sk_model=model,\n",
    "    artifact_path=\"Iris Mode\",\n",
    "    signature = signature,\n",
    "    input_example=x_train,\n",
    "    registered_model_name=\"Tracking-quickstart\"\n",
    "\t)\n",
    "\n",
    "```\n",
    "\n",
    "A new folder named `mlruns` is created which stores all the info about our experiments. We should not delete the `mlruns` folder.\n",
    "\n",
    "## **Tracking a ML Project with MLFlow**\n",
    "\n",
    "`project.ipynb`\n",
    "\n",
    "Let's create a sparate folder for our ML Project.\n",
    "\n",
    "Once we have setup our ML Project, now we will have to keep track of different performance metrics on the basis of our used hyperparameters. For which we will use `ML Flow`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089fb6bf",
   "metadata": {},
   "source": [
    "## **Inference of Model Artifacts**\n",
    "\n",
    "### **UI**\n",
    "\n",
    "`path` : Path of artifacts\n",
    "\n",
    "**Validate Before Deployment**\n",
    "\n",
    "As soon as we complete training our model, the model is saved as `model.pkl` in the `artifacts` but before using the model in the production we will need to validate it.\n",
    "\n",
    "For that the base code already provided in the UI only.\n",
    "\n",
    "```Py\n",
    "\n",
    "# Validate The Model\n",
    "\n",
    "import mlflow\n",
    "from mlflow.models import Model\n",
    "\n",
    "model_uri = 'runs:/cd866b98bcfb4235bbe3b225ece9fce9/Iris Mode'\n",
    "# The model is logged with an input example\n",
    "pyfunc_model = mlflow.pyfunc.load_model(model_uri)\n",
    "\n",
    "predictions = pyfunc_model.predict(x_test)\n",
    "\n",
    "predictions\n",
    "\n",
    "```\n",
    "\n",
    "`mlflow.pyfunc.load_model` loads the model as `Python's` generic function.\n",
    "\n",
    "## **Model Registry Tracking**\n",
    "\n",
    "Model Registry is a centralized model store, set of APIs, and UI to collaboratively manage the full lifecycke of an MLFlow Model. It provides model lineage (which MLFlow exps and runs produced the model), model versioning, model aliasing, model tagging and annotations.\n",
    "\n",
    "In the previous code, we directly saved (Registerd) the model without even validating if it the best model. As we provided `registered_model_name=\"Tracking-quickstart\"` argument in the `log_model` function which registers and maintains the model versioning.\n",
    "\n",
    "To avoid it we should not pass this parameter. If we not pass this parameter in the `UI` there will be a `Button` as `Register Model`. If the model has been registered then it would be `Model Registered` with it's version.\n",
    "\n",
    "How do we choose the best model? We need to compare the experiments and then find the experiment with the highest accuracy and then register that experiment.\n",
    "\n",
    "Okay, we've saved our best model but how are we going to predict from the saved best model?\n",
    "\n",
    "```Py\n",
    "\n",
    "# Inferencing the Model from the Model Registry (Prediction from the Best Model)\n",
    "\n",
    "# Inferencing the Model from the Model Registry (Prediction from the Best Model)\n",
    "\n",
    "import mlflow.sklearn\n",
    "\n",
    "model_name = 'Tracking-quickstart'\n",
    "model_version = '6' # Version of the best model {latest, number_version, ..}\n",
    "\n",
    "# Path for the model from the Model Registry\n",
    "model_uri = f\"models:/{model_name}/{model_version}\"\n",
    "\n",
    "model = mlflow.sklearn.load_model(model_uri)\n",
    "\n",
    "model.predict(x_test)\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff5dc54",
   "metadata": {},
   "source": [
    "## **Hosue Price Pred (MLFlow)**\n",
    "\n",
    "Refer to `ML_Project/Phase2(House).ipynb` file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bd2066",
   "metadata": {},
   "source": [
    "## **ANN with MLFlow**\n",
    "\n",
    "Refer to `ML_Flow(ANN_Project)`\n",
    "\n",
    "### **Pipeline**\n",
    "\n",
    "- Build an ANN Project\n",
    "\n",
    "- Run a hyperparameter sweep on a training script.\n",
    "\n",
    "- Compare the results of the runs in the MLFlow UI\n",
    "\n",
    "- Choose the best run and register it as a model\n",
    "\n",
    "- Deploy the Model to a REST API\n",
    "\n",
    "- Build a container image suitable for deployment to a cloud platform\n",
    "\n",
    "**Libraries**\n",
    "\n",
    "`keras`\n",
    "\n",
    "`tensorflow`\n",
    "\n",
    "`hyperopt` : Hyperparameter Tuining for the `ANN`\n",
    "\n",
    "**Documentation**\n",
    "\n",
    "[Hyperopt](https://hyperopt.github.io/hyperopt/)\n",
    "\n",
    "**Dataset**\n",
    "\n",
    "```Py\n",
    "\n",
    "# Data Wine Data\n",
    "\n",
    "data = pd.read_csv(\n",
    "  'https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv',\n",
    "  sep=';'\n",
    ")\n",
    "\n",
    "data\n",
    "\n",
    "```\n",
    "\n",
    "In the above data set, the `target`is the `quality {1-6}`. It is a classification task.\n",
    "\n",
    "Now, we will need to build an ANN to classify it.\n",
    "\n",
    "```Py\n",
    "\n",
    "\tmodel.compile(optimizer=keras.optimizers.SGD(\n",
    "\t\tlearning_rate=params['lr'],momentum=params[\"momentum\"]\n",
    "\t))\n",
    "\n",
    "```\n",
    "\n",
    "In the above, we change the `Dense` layers as an HyperParameter Tuining but it will take a lot of time. Instead we tune the `learning_rate` hyperparameter and `momemtum`.\n",
    "\n",
    "Now, we will try to train our model for the different combination of values of `learning_rate` and `momentum` and track each and every experiment for each combination.\n",
    "\n",
    "For the combination of different values of our `HyperParameters` we will use the `Hyperopt` library.\n",
    "\n",
    "Now once the model `compilation` code is written.\n",
    "\n",
    "```Py\n",
    "\n",
    "# ANN Model\n",
    "\n",
    "import mlflow.tensorflow\n",
    "\n",
    "\n",
    "def train_model(params, epochs, train_x, train_y, valid_x, valid_y, test_x, test_y):\n",
    "\n",
    "\t# Noramlization\n",
    "\tmean = np.mean(train_x,axis=0) # Mean of each col\n",
    "\tvar = np.var(train_x,axis=0) # Var of Each col\n",
    "\n",
    "\tmodel = keras.Sequential(\n",
    "\t\t[\n",
    "\t\t\tkeras.Input([train_x.shape[1]]),\n",
    "\t\t\tkeras.layers.Normalization(mean=mean,variance=var),\n",
    "\t\t\tkeras.layers.Dense(64,activation='relu'),\n",
    "\t\t\tkeras.layers.Dense(1) # Classification\n",
    "\t\t]\n",
    "\t)\n",
    "\n",
    "\t# Model Compile\n",
    "\tmodel.compile(optimizer=keras.optimizers.SGD(\n",
    "\t\tlearning_rate=params['lr'],momentum=params[\"momentum\"]\n",
    "\t),\n",
    "\tloss=\"mean_squared_error\",\n",
    "\tmetrics=[keras.metrics.RootMeanSquaredError()])\n",
    "\n",
    "\t# Train and Track the Hyperparam with MLFlow tracking\n",
    "\n",
    "\twith mlflow.start_run(nested=True): # As we are trying with multiple combination, nested = True\n",
    "\t\tmodel.fit(train_x,train_y,validation_data=(valid_x,valid_y),\n",
    "\t\t\t\t\t\tepochs=epochs,\n",
    "\t\t\t\t\t\tbatch_size=64)\n",
    "\n",
    "\t\t# Evaluate the model\n",
    "\t\teval_result = model.evaluate(valid_x, valid_y, batch_size=64)\n",
    "\n",
    "\t\teval_rmse = eval_result[1]\n",
    "\n",
    "\t\t# Log the params\n",
    "\t\tmlflow.log_param(params)\n",
    "\t\tmlflow.log_metric(\"Eval Rms\", eval_rmse)\n",
    "\n",
    "\t\t# Log the model\n",
    "\t\tmlflow.tensorflow.log_model(\n",
    "\t\t\tmodel,\n",
    "\t\t\t\"model\",\n",
    "\t\t\tsignature=signature\n",
    "\t\t)\n",
    "\n",
    "\t\treturn {\n",
    "\t\t\t'loss': eval_rmse,\n",
    "\t\t\t'status': STATUS_OK,\n",
    "\t\t\t'model': model\n",
    "\t\t}\n",
    "```\n",
    "\n",
    "We will need to create an `objective` function for the `HyperOpt`.\n",
    "\n",
    "```Py\n",
    "\n",
    "# Objective Function for Hyperopt\n",
    "\n",
    "def objective(params):\n",
    "\n",
    "  # MlFlow will track the params and results for each run\n",
    "\n",
    "  result = train_model(\n",
    "    params,\n",
    "    epochs=3,\n",
    "    train_x=train_x,\n",
    "    train_y=train_y,\n",
    "    valid_x=valid_x,\n",
    "    valid_y=valid_y,\n",
    "    test_x=test_x,\n",
    "    test_y=test_y\n",
    "  )\n",
    "\n",
    "  return result\n",
    "\n",
    "```\n",
    "\n",
    "**Parameters**\n",
    "\n",
    "```Py\n",
    "\n",
    "# Set all the parameters\n",
    "\n",
    "space = {\n",
    " 'lr': hp.loguniform('lr',np.log(1e-5),np.log(1e-1)),\n",
    " 'momentum': hp.uniform(\"momentum\",0.0,1.0)\n",
    "}\n",
    "\n",
    "```\n",
    "\n",
    "**Parent Run**\n",
    "\n",
    "```Py\n",
    "\n",
    "# Set Exp\n",
    "\n",
    "import mlflow.tensorflow\n",
    "\n",
    "\n",
    "mlflow.set_experiment(\"/wine-quality\")\n",
    "\n",
    "# Create another run so that the nested run will work\n",
    "with mlflow.start_run():\n",
    "\n",
    "  # Conduct Hyperparameter search using Hyperopt\n",
    "  trails = Trials()\n",
    "  best = fmin(\n",
    "    fn=objective,\n",
    "    space=space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=4,\n",
    "    trials=trails\n",
    "  )\n",
    "\n",
    "  # Fetch the details of the best run\n",
    "  best_run = sorted(trails.results, key=lambda x:x['loss'])[0]\n",
    "\n",
    "  # Log the best parameters, loss and model\n",
    "  for key, value in best.items():\n",
    "    mlflow.log_param(key, value)\n",
    "\n",
    "  mlflow.log_metric(\"Eval_RMSE\", best_run['loss'])\n",
    "  mlflow.tensorflow.log_model(\n",
    "    best_run['model'],\n",
    "    \"model\",\n",
    "    signature=signature,\n",
    "  )\n",
    "\n",
    "  # Print out the best params and loss\n",
    "  print(f\"Best Param: {best}\")\n",
    "  print(f\"Best Eval EMSE: {best_run['loss']}\")\n",
    "\n",
    "```\n",
    "\n",
    "The `fmin` function calls the `objective` function 4 times as `max_eval=4`. Each call trains a new model using new hyperparameter combination. The `trials.results` contains results of all runs.\n",
    "\n",
    "**Inferencing (Load and Predict)**\n",
    "\n",
    "```Py\n",
    "\n",
    "# Inferencing Model\n",
    "\n",
    "import mlflow\n",
    "\n",
    "model_uri = 'runs:/d6afa02fdf47443bb97a85e0068fd121/model'\n",
    "\n",
    "loaded_model = mlflow.pyfunc.load_model(model_uri)\n",
    "predictions = loaded_model.predict(test_x)\n",
    "predictions\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29975a1c",
   "metadata": {},
   "source": [
    "## **DVC and DagsHub**\n",
    "\n",
    "### **DVC (Data Version Control)**\n",
    "\n",
    "For data verioning data.\n",
    "\n",
    "If you store and process data files or datasets to produce other data or machine learning models, and you want to\n",
    "\n",
    "- track and save data and machine learning models the same way you capture code;\n",
    "\n",
    "- create and switch between versions of data and ML models easily;\n",
    "\n",
    "- understand how datasets and ML artifacts were built in the first place;\n",
    "\n",
    "- compare model metrics among experiments;\n",
    "\n",
    "- adopt engineering tools and best practices in data science projects;\n",
    "\n",
    "`pip install dvc`\n",
    "\n",
    "**Initialize the DVC**\n",
    "\n",
    "For it to be initialiZed the `git` should be initialized\n",
    "\n",
    "`dvc init`\n",
    "\n",
    "`.dvc` folder is created.\n",
    "\n",
    "Note that git should not track the `Dataset` folder.\n",
    "\n",
    "Now, add the files or data that you want to keep track of. `dvc add location/file.txt`\n",
    "\n",
    "After the there is change in the Dataset, always add the file using `dvc add 'Datasets(DVC)/Day1.txt'`\n",
    "\n",
    "Then we will only track the hash value (.dvc) file and .gitignore file from the dataset not the dataset. `git add 'Datasets(DVC)/Day1.txt.dvc' 'Datasets(DVC)/.gitignore'`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e1d8f0",
   "metadata": {},
   "source": [
    "### **Next Day**\n",
    "\n",
    "**Revise Everything Before Starting**\n",
    "\n",
    "https://www.udemy.com/course/complete-mlops-bootcamp-with-10-end-to-end-ml-projects/learn/lecture/46093051#overview\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
