{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2eae0d8b",
   "metadata": {},
   "source": [
    "## **Working of with Keyowrd**\n",
    "\n",
    "[GPT_Explanation](https://chatgpt.com/share/681f4657-6520-8006-b695-a215a1783899)\n",
    "\n",
    "[Real_Python_Implementation](https://www.youtube.com/watch?v=iba-I4CrmyA)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3429f5bc",
   "metadata": {},
   "source": [
    "## **Before Starting Project**\n",
    "\n",
    "`source mlflow_env/bin/activate`\n",
    "\n",
    "`mlflow ui`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a79da5f",
   "metadata": {},
   "source": [
    "## **ML Flow**\n",
    "\n",
    "Open-source. We can track our Machine Learning project such as performance metrices etc.\n",
    "\n",
    "## **Lifecycle of a Data Science Project**\n",
    "\n",
    "**Data Pre**\n",
    "\n",
    "**EDA**\n",
    "\n",
    "**Feature Eng**\n",
    "\n",
    "**Model Training**\n",
    "\n",
    "**Model Validation**\n",
    "\n",
    "**Deployment**\n",
    "\n",
    "**Monitoring**\n",
    "\n",
    "## **How ML Flow is used by Data Scientist**\n",
    "\n",
    "- Experiment Tracking\n",
    "\n",
    "- Hypothesis Testing in EDA\n",
    "\n",
    "- Code Structuring (Pipeline)\n",
    "\n",
    "- Model Packaging and Dependency Management\n",
    "\n",
    "- Evaluating Hyperparameter : Track every combination of Hyperparameter\n",
    "\n",
    "- Compare the results of model and deploy the best performing model\n",
    "\n",
    "## **How ML Flow is used by ML Engineeer**\n",
    "\n",
    "- Manage the lifecycle of trained models both pre and post deployment\n",
    "\n",
    "- Deploy models security to the production env\n",
    "\n",
    "- Manage Deployment Dependencies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d72f24",
   "metadata": {},
   "source": [
    "## **ML Flow Starter**\n",
    "\n",
    "### **ML Flow Tracking Server**\n",
    "\n",
    "For tracking our experiments we need to create a server\n",
    "\n",
    "To start the server we use `mlflow ui`.\n",
    "\n",
    "Then we will need to provide the tracking UI so that everything is tracked by MLFlow. `mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")`\n",
    "\n",
    "Then to log our performance metrices we will use as below:\n",
    "\n",
    "```py\n",
    "\n",
    "mlflow.set_experiment(\"Day2\")\n",
    "\n",
    "# Start the MLFlow Run\n",
    "\n",
    "with mlflow.start_run():\n",
    "  # Log the hyperparameters\n",
    "  mlflow.log_params(params)\n",
    "\n",
    "\t# Log te accuracy metrics\n",
    "  mlflow.log_metric(\"Accuracy\",accuracy)\n",
    "\n",
    "\t# Set tag that we can use to remind ourselves what this run was for\n",
    "  mlflow.set_tag(\"Training Info\", \"Basic LR Model for Iris Data\")\n",
    "\n",
    "\t# Infer the model signature\n",
    "  signature = infer_signature(x_train, model.predict(x_train))\n",
    "\n",
    "\t# Log the model\n",
    "  model_info = mlflow.sklearn.log_model(\n",
    "    sk_model=model,\n",
    "    artifact_path=\"Iris Mode\",\n",
    "    signature = signature,\n",
    "    input_example=x_train,\n",
    "    registered_model_name=\"Tracking-quickstart\"\n",
    "\t)\n",
    "\n",
    "```\n",
    "\n",
    "A new folder named `mlruns` is created which stores all the info about our experiments. We should not delete the `mlruns` folder.\n",
    "\n",
    "## **Tracking a ML Project with MLFlow**\n",
    "\n",
    "`project.ipynb`\n",
    "\n",
    "Let's create a sparate folder for our ML Project.\n",
    "\n",
    "Once we have setup our ML Project, now we will have to keep track of different performance metrics on the basis of our used hyperparameters. For which we will use `ML Flow`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089fb6bf",
   "metadata": {},
   "source": [
    "## **Inference of Model Artifacts**\n",
    "\n",
    "### **UI**\n",
    "\n",
    "`path` : Path of artifacts\n",
    "\n",
    "**Validate Before Deployment**\n",
    "\n",
    "As soon as we complete training our model, the model is saved as `model.pkl` in the `artifacts` but before using the model in the production we will need to validate it.\n",
    "\n",
    "For that the base code already provided in the UI only.\n",
    "\n",
    "```Py\n",
    "\n",
    "# Validate The Model\n",
    "\n",
    "import mlflow\n",
    "from mlflow.models import Model\n",
    "\n",
    "model_uri = 'runs:/cd866b98bcfb4235bbe3b225ece9fce9/Iris Mode'\n",
    "# The model is logged with an input example\n",
    "pyfunc_model = mlflow.pyfunc.load_model(model_uri)\n",
    "\n",
    "predictions = pyfunc_model.predict(x_test)\n",
    "\n",
    "predictions\n",
    "\n",
    "```\n",
    "\n",
    "`mlflow.pyfunc.load_model` loads the model as `Python's` generic function.\n",
    "\n",
    "## **Model Registry Tracking**\n",
    "\n",
    "Model Registry is a centralized model store, set of APIs, and UI to collaboratively manage the full lifecycke of an MLFlow Model. It provides model lineage (which MLFlow exps and runs produced the model), model versioning, model aliasing, model tagging and annotations.\n",
    "\n",
    "In the previous code, we directly saved (Registerd) the model without even validating if it the best model. As we provided `registered_model_name=\"Tracking-quickstart\"` argument in the `log_model` function which registers and maintains the model versioning.\n",
    "\n",
    "To avoid it we should not pass this parameter. If we not pass this parameter in the `UI` there will be a `Button` as `Register Model`. If the model has been registered then it would be `Model Registered` with it's version.\n",
    "\n",
    "How do we choose the best model? We need to compare the experiments and then find the experiment with the highest accuracy and then register that experiment.\n",
    "\n",
    "Okay, we've saved our best model but how are we going to predict from the saved best model?\n",
    "\n",
    "```Py\n",
    "\n",
    "# Inferencing the Model from the Model Registry (Prediction from the Best Model)\n",
    "\n",
    "# Inferencing the Model from the Model Registry (Prediction from the Best Model)\n",
    "\n",
    "import mlflow.sklearn\n",
    "\n",
    "model_name = 'Tracking-quickstart'\n",
    "model_version = '6' # Version of the best model {latest, number_version, ..}\n",
    "\n",
    "# Path for the model from the Model Registry\n",
    "model_uri = f\"models:/{model_name}/{model_version}\"\n",
    "\n",
    "model = mlflow.sklearn.load_model(model_uri)\n",
    "\n",
    "model.predict(x_test)\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff5dc54",
   "metadata": {},
   "source": [
    "## **Hosue Price Pred (MLFlow)**\n",
    "\n",
    "Refer to `ML_Project/Phase2(House).ipynb` file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bd2066",
   "metadata": {},
   "source": [
    "## **ANN with MLFlow**\n",
    "\n",
    "Refer to `ML_Flow(ANN_Project)`\n",
    "\n",
    "### **Pipeline**\n",
    "\n",
    "- Build an ANN Project\n",
    "\n",
    "- Run a hyperparameter sweep on a training script.\n",
    "\n",
    "- Compare the results of the runs in the MLFlow UI\n",
    "\n",
    "- Choose the best run and register it as a model\n",
    "\n",
    "- Deploy the Model to a REST API\n",
    "\n",
    "- Build a container image suitable for deployment to a cloud platform\n",
    "\n",
    "**Libraries**\n",
    "\n",
    "`keras`\n",
    "\n",
    "`tensorflow`\n",
    "\n",
    "`hyperopt` : Hyperparameter Tuining for the `ANN`\n",
    "\n",
    "**Documentation**\n",
    "\n",
    "[Hyperopt](https://hyperopt.github.io/hyperopt/)\n",
    "\n",
    "**Dataset**\n",
    "\n",
    "```Py\n",
    "\n",
    "# Data Wine Data\n",
    "\n",
    "data = pd.read_csv(\n",
    "  'https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv',\n",
    "  sep=';'\n",
    ")\n",
    "\n",
    "data\n",
    "\n",
    "```\n",
    "\n",
    "In the above data set, the `target`is the `quality {1-6}`. It is a classification task.\n",
    "\n",
    "Now, we will need to build an ANN to classify it.\n",
    "\n",
    "```Py\n",
    "\n",
    "\tmodel.compile(optimizer=keras.optimizers.SGD(\n",
    "\t\tlearning_rate=params['lr'],momentum=params[\"momentum\"]\n",
    "\t))\n",
    "\n",
    "```\n",
    "\n",
    "In the above, we change the `Dense` layers as an HyperParameter Tuining but it will take a lot of time. Instead we tune the `learning_rate` hyperparameter and `momemtum`.\n",
    "\n",
    "Now, we will try to train our model for the different combination of values of `learning_rate` and `momentum` and track each and every experiment for each combination.\n",
    "\n",
    "For the combination of different values of our `HyperParameters` we will use the `Hyperopt` library.\n",
    "\n",
    "Now once the model `compilation` code is written.\n",
    "\n",
    "```Py\n",
    "\n",
    "# ANN Model\n",
    "\n",
    "import mlflow.tensorflow\n",
    "\n",
    "\n",
    "def train_model(params, epochs, train_x, train_y, valid_x, valid_y, test_x, test_y):\n",
    "\n",
    "\t# Noramlization\n",
    "\tmean = np.mean(train_x,axis=0) # Mean of each col\n",
    "\tvar = np.var(train_x,axis=0) # Var of Each col\n",
    "\n",
    "\tmodel = keras.Sequential(\n",
    "\t\t[\n",
    "\t\t\tkeras.Input([train_x.shape[1]]),\n",
    "\t\t\tkeras.layers.Normalization(mean=mean,variance=var),\n",
    "\t\t\tkeras.layers.Dense(64,activation='relu'),\n",
    "\t\t\tkeras.layers.Dense(1) # Classification\n",
    "\t\t]\n",
    "\t)\n",
    "\n",
    "\t# Model Compile\n",
    "\tmodel.compile(optimizer=keras.optimizers.SGD(\n",
    "\t\tlearning_rate=params['lr'],momentum=params[\"momentum\"]\n",
    "\t),\n",
    "\tloss=\"mean_squared_error\",\n",
    "\tmetrics=[keras.metrics.RootMeanSquaredError()])\n",
    "\n",
    "\t# Train and Track the Hyperparam with MLFlow tracking\n",
    "\n",
    "\twith mlflow.start_run(nested=True): # As we are trying with multiple combination, nested = True\n",
    "\t\tmodel.fit(train_x,train_y,validation_data=(valid_x,valid_y),\n",
    "\t\t\t\t\t\tepochs=epochs,\n",
    "\t\t\t\t\t\tbatch_size=64)\n",
    "\n",
    "\t\t# Evaluate the model\n",
    "\t\teval_result = model.evaluate(valid_x, valid_y, batch_size=64)\n",
    "\n",
    "\t\teval_rmse = eval_result[1]\n",
    "\n",
    "\t\t# Log the params\n",
    "\t\tmlflow.log_param(params)\n",
    "\t\tmlflow.log_metric(\"Eval Rms\", eval_rmse)\n",
    "\n",
    "\t\t# Log the model\n",
    "\t\tmlflow.tensorflow.log_model(\n",
    "\t\t\tmodel,\n",
    "\t\t\t\"model\",\n",
    "\t\t\tsignature=signature\n",
    "\t\t)\n",
    "\n",
    "\t\treturn {\n",
    "\t\t\t'loss': eval_rmse,\n",
    "\t\t\t'status': STATUS_OK,\n",
    "\t\t\t'model': model\n",
    "\t\t}\n",
    "```\n",
    "\n",
    "We will need to create an `objective` function for the `HyperOpt`.\n",
    "\n",
    "```Py\n",
    "\n",
    "# Objective Function for Hyperopt\n",
    "\n",
    "def objective(params):\n",
    "\n",
    "  # MlFlow will track the params and results for each run\n",
    "\n",
    "  result = train_model(\n",
    "    params,\n",
    "    epochs=3,\n",
    "    train_x=train_x,\n",
    "    train_y=train_y,\n",
    "    valid_x=valid_x,\n",
    "    valid_y=valid_y,\n",
    "    test_x=test_x,\n",
    "    test_y=test_y\n",
    "  )\n",
    "\n",
    "  return result\n",
    "\n",
    "```\n",
    "\n",
    "**Parameters**\n",
    "\n",
    "```Py\n",
    "\n",
    "# Set all the parameters\n",
    "\n",
    "space = {\n",
    " 'lr': hp.loguniform('lr',np.log(1e-5),np.log(1e-1)),\n",
    " 'momentum': hp.uniform(\"momentum\",0.0,1.0)\n",
    "}\n",
    "\n",
    "```\n",
    "\n",
    "**Parent Run**\n",
    "\n",
    "```Py\n",
    "\n",
    "# Set Exp\n",
    "\n",
    "import mlflow.tensorflow\n",
    "\n",
    "\n",
    "mlflow.set_experiment(\"/wine-quality\")\n",
    "\n",
    "# Create another run so that the nested run will work\n",
    "with mlflow.start_run():\n",
    "\n",
    "  # Conduct Hyperparameter search using Hyperopt\n",
    "  trails = Trials()\n",
    "  best = fmin(\n",
    "    fn=objective,\n",
    "    space=space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=4,\n",
    "    trials=trails\n",
    "  )\n",
    "\n",
    "  # Fetch the details of the best run\n",
    "  best_run = sorted(trails.results, key=lambda x:x['loss'])[0]\n",
    "\n",
    "  # Log the best parameters, loss and model\n",
    "  for key, value in best.items():\n",
    "    mlflow.log_param(key, value)\n",
    "\n",
    "  mlflow.log_metric(\"Eval_RMSE\", best_run['loss'])\n",
    "  mlflow.tensorflow.log_model(\n",
    "    best_run['model'],\n",
    "    \"model\",\n",
    "    signature=signature,\n",
    "  )\n",
    "\n",
    "  # Print out the best params and loss\n",
    "  print(f\"Best Param: {best}\")\n",
    "  print(f\"Best Eval EMSE: {best_run['loss']}\")\n",
    "\n",
    "```\n",
    "\n",
    "The `fmin` function calls the `objective` function 4 times as `max_eval=4`. Each call trains a new model using new hyperparameter combination. The `trials.results` contains results of all runs.\n",
    "\n",
    "**Inferencing (Load and Predict)**\n",
    "\n",
    "```Py\n",
    "\n",
    "# Inferencing Model\n",
    "\n",
    "import mlflow\n",
    "\n",
    "model_uri = 'runs:/d6afa02fdf47443bb97a85e0068fd121/model'\n",
    "\n",
    "loaded_model = mlflow.pyfunc.load_model(model_uri)\n",
    "predictions = loaded_model.predict(test_x)\n",
    "predictions\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29975a1c",
   "metadata": {},
   "source": [
    "## **DVC (Data Version Control)**\n",
    "\n",
    "For data verioning data.\n",
    "\n",
    "If you store and process data files or datasets to produce other data or machine learning models, and you want to\n",
    "\n",
    "- track and save data and machine learning models the same way you capture code;\n",
    "\n",
    "- create and switch between versions of data and ML models easily;\n",
    "\n",
    "- understand how datasets and ML artifacts were built in the first place;\n",
    "\n",
    "- compare model metrics among experiments;\n",
    "\n",
    "- adopt engineering tools and best practices in data science projects;\n",
    "\n",
    "`pip install dvc`\n",
    "\n",
    "**Initialize the DVC**\n",
    "\n",
    "For it to be initialiZed the `git` should be initialized\n",
    "\n",
    "`dvc init`\n",
    "\n",
    "`.dvc` folder is created.\n",
    "\n",
    "Note that git should not track the `Dataset` folder.\n",
    "\n",
    "Now, add the files or data that you want to keep track of. `dvc add location/file.txt`\n",
    "\n",
    "After the there is change in the Dataset, always add the file using `dvc add 'Datasets(DVC)/Day1.txt'`\n",
    "\n",
    "Then we will only track the hash value (.dvc) file and .gitignore file from the dataset not the dataset. `git add 'Datasets(DVC)/Day1.txt.dvc' 'Datasets(DVC)/.gitignore'`\n",
    "\n",
    "**Switching Between Previous Versions of the Data**\n",
    "\n",
    "First find the commit id in which you want to switch to.\n",
    "\n",
    "`git log` then copy your commit id.\n",
    "\n",
    "Then,\n",
    "\n",
    "`git checkout commit_id`\n",
    "\n",
    "`dvc checkout`\n",
    "\n",
    "You are in the previous time stamp.\n",
    "\n",
    "Then setup the security creds\n",
    "\n",
    "```bash\n",
    "\n",
    "dvc remote modify origin --local access_key_id bee4f040f555191f8c8fb05458249bed6f421f06\n",
    "dvc remote modify origin --local secret_access_key bee4f040f555191f8c8fb05458249bed6f421f06\n",
    "\n",
    "```\n",
    "\n",
    "```bash\n",
    "\n",
    "(mlflow_env) toni-birat@tonibirat:/media/toni-birat/New Volume/ML_Flow_Complete$ dvc remote list\n",
    "origin  s3://dvc\n",
    "(mlflow_env) toni-birat@tonibirat:/media/toni-birat/New Volume/ML_Flow_Complete$\n",
    "\n",
    "```\n",
    "\n",
    "Now, to pull and push the data we will need another library i.e. `dvc_s3`\n",
    "\n",
    "DagsHub provide a default of `10GB` S3 storage for free.\n",
    "\n",
    "Then. `dvc pull -r origin`. Origin is the name of the remote location in the DagsHub (Link)\n",
    "\n",
    "Then, `dvc push -r origin` and `git push origin master`. Our dataset would be tracked.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822a32a4",
   "metadata": {},
   "source": [
    "## **Dags Hub**\n",
    "\n",
    "Till now we've tracked out Machine Learning task locally, but with `DagsHub` we can host our MlFlow tracking in a remote location. With this many people can access it and compare against thier model.\n",
    "\n",
    "It provide features like Version Contro for Code, Version Control for Data (DVC) and Experiment Tracking.\n",
    "\n",
    "To track the data\n",
    "\n",
    "```Text\n",
    "\n",
    "dvc remote add origin s3://dvc\n",
    "dvc remote modify origin endpointurl https://dagshub.com/ToniBirat7/ML_Flow_End_To_End_ML.s3\n",
    "\n",
    "```\n",
    "\n",
    "**This Didnot Work**\n",
    "\n",
    "Try creating a new project.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4c188e",
   "metadata": {},
   "source": [
    "## **Production Style Project Structure**\n",
    "\n",
    "**Name** : First End to End Project\n",
    "\n",
    "The project is created as a sub moudle of the main project.\n",
    "\n",
    "A `.gitmodules` file is created to track the modules. Also, there is a independent repo for this project.\n",
    "\n",
    "`git@github.com:ToniBirat7/First_End_To_End_ML_Project.git`\n",
    "\n",
    "**Project Structure**\n",
    "\n",
    "```Text\n",
    "First_End_To_End_ML_Project\n",
    "â”œâ”€â”€ .gitignore\n",
    "â”œâ”€â”€ README.md\n",
    "â”œâ”€â”€ params.yaml\n",
    "â”œâ”€â”€ src\n",
    "â”‚   â”œâ”€â”€ __init__.pyS\n",
    "â”‚   â”‚   â”œâ”€â”€ preprocess.py\n",
    "â”‚   â”‚   â”œâ”€â”€ evaluate.py\n",
    "â”‚   â”‚   â”œâ”€â”€ train.py\n",
    "â”œâ”€â”€ params.yaml\n",
    "â”œâ”€â”€ requirements.txt\n",
    "```\n",
    "\n",
    "**`params.yaml`**\n",
    "\n",
    "The `params.yaml` file is used to store the hyperparameters and other parameters that we will use in our project. It is a YAML file.\n",
    "\n",
    "```yaml\n",
    "# params.yaml\n",
    "model:\n",
    "  name: \"Pima Indian Diabetes\"\n",
    "  version: \"1.0\"\n",
    "model_config:\n",
    "  train_size: 0.8\n",
    "  random_state: 42\n",
    "  test_size: 0.2\n",
    "model_params:\n",
    "  # Hyperparameters for the model\n",
    "  max_depth: 5\n",
    "  n_estimators: 100\n",
    "  learning_rate: 0.01\n",
    "model_type: \"Random Forest\"\n",
    "model:\n",
    "  # Hyperparameters for the model\n",
    "  C: 0.1\n",
    "  max_iter: 100\n",
    "  random_state: 42\n",
    "model:\n",
    "  # Hyperparameters for the model\n",
    "  type: \"Logistic Regression\"\n",
    "  params:\n",
    "    C: 0.1\n",
    "    max_iter: 100\n",
    "    random_state: 42\n",
    "data:\n",
    "  # Data parameters\n",
    "  source: \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
    "  columns:\n",
    "    - \"Pregnancies\"\n",
    "    - \"Glucose\"\n",
    "    - \"BloodPressure\"\n",
    "    - \"SkinThickness\"\n",
    "    - \"Insulin\"\n",
    "    - \"BMI\"\n",
    "    - \"DiabetesPedigreeFunction\"\n",
    "    - \"Age\"\n",
    "    - \"Outcome\"\n",
    "  target: \"Outcome\"\n",
    "```\n",
    "\n",
    "**`src/preprocess.py`**\n",
    "\n",
    "This file is used to preprocess the data. It contains functions to load the data, split the data into train and test sets, and preprocess the data.\n",
    "\n",
    "```python\n",
    "\n",
    "import pandas as pd\n",
    "import sys\n",
    "import yaml\n",
    "import os\n",
    "\n",
    "# Params\n",
    "\n",
    "params = yaml.safe_load(open('params.yaml'))['preprocess']\n",
    "\n",
    "def preprocess_data(input_file, output_file):\n",
    "  \"\"\"\n",
    "  Preprocess the data by reading from input_file, performing necessary transformations,\n",
    "  and saving the processed data to output_file.\n",
    "  \"\"\"\n",
    "  # Read the data\n",
    "  df = pd.read_csv(input_file, header=None)\n",
    "\n",
    "  # Preprocessing : But the data is already clean, so we will just rename the columns\n",
    "\n",
    "  os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "\n",
    "  # Save the processed data\n",
    "  df.to_csv(output_file, index=False)\n",
    "  print(f\"Processed data saved to {output_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  preprocess_data(\n",
    "    input_file=params['input'],\n",
    "    output_file=params['output']\n",
    "  )\n",
    "\n",
    "```\n",
    "\n",
    "**`src/train.py`**\n",
    "\n",
    "```Py\n",
    "\n",
    "import mlflow.sklearn\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from mlflow.models import infer_signature\n",
    "from dotenv import load_dotenv\n",
    "from urllib.parse import urlparse\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "import yaml\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import os\n",
    "print(\"CWD:\", os.getcwd())\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Now use the environment variables\n",
    "tracking_uri = os.getenv('MLFLOW_TRACKING_URI')\n",
    "experiment_name = os.getenv('MLFLOW_EXPERIMENT_NAME')\n",
    "username = os.getenv('MLFLOW_TRACKING_USERNAME')\n",
    "password = os.getenv('MLFLOW_TRACKING_PASSWORD')\n",
    "\n",
    "# Set env vars for MLflow auth\n",
    "os.environ[\"MLFLOW_TRACKING_USERNAME\"] = username\n",
    "os.environ[\"MLFLOW_TRACKING_PASSWORD\"] = password\n",
    "\n",
    "print(f\"Tracking URI: {tracking_uri}\")\n",
    "\n",
    "def hyperparameter_tuning(X_train, y_train, param_grid):\n",
    "  \"\"\"\n",
    "  Perform hyperparameter tuning for the RandomForestClassifier.\n",
    "  \"\"\"\n",
    "  # For simplicity, we will use default parameters\n",
    "  model = RandomForestClassifier()\n",
    "  grid_search = GridSearchCV(\n",
    "    estimator=model,\n",
    "    param_grid=param_grid,\n",
    "    cv=3,\n",
    "    n_jobs=-1,\n",
    "    verbose=2,\n",
    "  )\n",
    "  grid_search.fit(X_train, y_train)\n",
    "  print(f\"\\nBest parameters: {grid_search.best_params_}\")\n",
    "  return grid_search\n",
    "\n",
    "# Load all parameters\n",
    "def load_training_params():\n",
    "  with open('params.yaml', 'r') as file:\n",
    "    params = yaml.safe_load(file)['train']\n",
    "  return params\n",
    "\n",
    "\n",
    "def train_model(params,model_path):\n",
    "  \"\"\"\n",
    "  Train the RandomForestClassifier model with the given training data and parameters.\n",
    "  \"\"\"\n",
    "  path = params['input']\n",
    "  df = pd.read_csv(path)\n",
    "\n",
    "  print(\"\\n\" + \"=\"*50)\n",
    "  print(\"DATA LOADING AND PREPROCESSING\")\n",
    "  print(\"=\"*50)\n",
    "  print(f\"Columns in DataFrame: {df.columns.tolist()}\")\n",
    "\n",
    "  X = df.drop(columns=['Outcome'])\n",
    "  y = df['Outcome']\n",
    "\n",
    "  print(f\"\\nData loaded from: {path}\")\n",
    "  print(f\"Dataset shape: {df.shape}\")\n",
    "  print(f\"Features shape: {X.shape}\")\n",
    "  print(f\"Labels shape: {y.shape}\")\n",
    "  print(\"\\nStarting Train Test Split...\")\n",
    "\n",
    "  # Split the data into training and test sets\n",
    "  if 'test_size' not in params:\n",
    "    params['test_size'] = 0.2  # Default test size if not specified\n",
    "  if 'random_state' not in params:\n",
    "    params['random_state'] = 42  # Default random state if not specified\n",
    "\n",
    "  # Ensure the input data is in the correct format\n",
    "  if not isinstance(X, pd.DataFrame):\n",
    "    raise ValueError(\"Input features X must be a pandas DataFrame.\")\n",
    "  if not isinstance(y, pd.Series):\n",
    "    raise ValueError(\"Input labels y must be a pandas Series.\")\n",
    "  if X.empty or y.empty:\n",
    "    raise ValueError(\"Input features X and labels y cannot be empty.\")\n",
    "  if X.shape[0] != y.shape[0]:\n",
    "    raise ValueError(\"Input features X and labels y must have the same number of samples.\")\n",
    "\n",
    "  print(f\"\\nSplitting data with:\")\n",
    "  print(f\"  - Test size: {params['test_size']}\")\n",
    "  print(f\"  - Random state: {params['random_state']}\")\n",
    "\n",
    "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=params['test_size'], random_state=42)\n",
    "\n",
    "  print(f\"\\nTrain/Test Split Results:\")\n",
    "  print(f\"  - Training data shape: {X_train.shape}\")\n",
    "  print(f\"  - Test data shape: {X_test.shape}\")\n",
    "  print(f\"  - Training labels shape: {y_train.shape}\")\n",
    "  print(f\"  - Test labels shape: {y_test.shape}\")\n",
    "\n",
    "  # Set the signature for the model\n",
    "  signature = infer_signature(X_train, y_train)\n",
    "  print(f\"\\nModel signature inferred successfully.\")\n",
    "\n",
    "  # Perform hyperparameter tuning\n",
    "  print(\"\\n\" + \"=\"*50)\n",
    "  print(\"HYPERPARAMETER TUNING\")\n",
    "  print(\"=\"*50)\n",
    "\n",
    "  # Set the tracking URI and experiment name\n",
    "  print(f\"\\nMLflow Configuration:\")\n",
    "  print(f\"  - Tracking URI: {tracking_uri}\")\n",
    "  print(f\"  - Experiment name: {experiment_name}\")\n",
    "  mlflow.set_tracking_uri(tracking_uri)\n",
    "\n",
    "  # Set the experiment name\n",
    "  if username and password:\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "    print(\"  - Authentication: Enabled\")\n",
    "  else:\n",
    "    print(\"  - Authentication: No credentials provided for MLflow tracking server\")\n",
    "\n",
    "  # Start an MLflow run\n",
    "  with mlflow.start_run():\n",
    "    print(f\"\\nâœ“ MLflow run started successfully\")\n",
    "\n",
    "    # Perform hyperparameter tuning\n",
    "    param_grid = {\n",
    "      'n_estimators': [100, 200],\n",
    "      'max_depth': [None, 10, 20],\n",
    "      'min_samples_split': [2, 5],\n",
    "      'min_samples_leaf': [1, 2]\n",
    "    }\n",
    "\n",
    "    grid_search = hyperparameter_tuning(X_train, y_train, param_grid)\n",
    "    print(f\"\\nâœ“ Hyperparameter tuning completed\")\n",
    "    best_model = grid_search.best_estimator_\n",
    "    print(f\"âœ“ Best model found: {best_model}\")\n",
    "\n",
    "    # Train the model\n",
    "    best_model.fit(X_train, y_train)\n",
    "    print(f\"\\nâœ“ Model training completed\")\n",
    "\n",
    "    # Evaluate the model\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"MODEL EVALUATION\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Model Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    # Log metrics\n",
    "    mlflow.log_metric('accuracy', accuracy)\n",
    "\n",
    "    # Log parameters\n",
    "    print(f\"\\nLogging parameters to MLflow...\")\n",
    "    print(f\"Best parameters found: {grid_search.best_params_}\")\n",
    "    for key, value in grid_search.best_params_.items():\n",
    "      mlflow.log_param(key, value)\n",
    "    print(f\"âœ“ Parameters logged successfully\")\n",
    "\n",
    "    # Log the confusion matrix and classification report as text files in the Artifacts\n",
    "    print(f\"\\nLogging evaluation artifacts...\")\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    cr = classification_report(y_test, y_pred, output_dict=True)\n",
    "    mlflow.log_text(str(cm), \"confusion_matrix.txt\")\n",
    "    mlflow.log_text(str(cr), \"classification_report.txt\")\n",
    "    print(f\"âœ“ Confusion matrix and classification report logged\")\n",
    "\n",
    "    # Log the model signature\n",
    "    print(f\"âœ“ Model signature logged\")\n",
    "    mlflow.log_param(\"signature\", str(signature))\n",
    "\n",
    "    # Log the model\n",
    "    print(f\"\\n\" + \"=\"*50)\n",
    "    print(\"MODEL LOGGING\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    tracking_uri_type_store = urlparse(mlflow.get_tracking_uri()).scheme\n",
    "    print(f\"Tracking URI type: {tracking_uri_type_store}\")\n",
    "\n",
    "    if tracking_uri_type_store != 'file':\n",
    "      print(f\"Using MLflow server for model logging...\")\n",
    "      mlflow.sklearn.log_model(\n",
    "          sk_model=best_model,\n",
    "          artifact_path=\"model\",\n",
    "          signature=signature,\n",
    "          registered_model_name=params['model_name']\n",
    "      )\n",
    "    else:\n",
    "      print(f\"Using local file system for model logging...\")\n",
    "      mlflow.sklearn.log_model(\n",
    "          sk_model=best_model,\n",
    "          artifact_path=\"model\",\n",
    "          signature=signature\n",
    "      )\n",
    "    print(f\"âœ“ Model logged to MLflow successfully\")\n",
    "\n",
    "    # Ensure the directory exists\n",
    "    os.makedirs(os.path.dirname(model_path), exist_ok=True)\n",
    "    print(f\"\\nSaving model locally...\")\n",
    "    print(f\"Model path: {model_path}\")\n",
    "\n",
    "    # Save the model to a file\n",
    "    with open(model_path, 'wb') as f:\n",
    "      pickle.dump(best_model, f)\n",
    "\n",
    "    print(f\"âœ“ Model saved locally to {model_path}\")\n",
    "\n",
    "    print(f\"\\n\" + \"=\"*50)\n",
    "    print(\"TRAINING COMPLETED SUCCESSFULLY\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  params = load_training_params()\n",
    "  model_path = params['output']\n",
    "\n",
    "  # Ensure the model path is set correctly\n",
    "  if not model_path:\n",
    "    raise ValueError(\"Model path is not set in params.yaml.\")\n",
    "\n",
    "  print(f\"Model will be saved to {model_path}\")\n",
    "\n",
    "  train_model(params, model_path)\n",
    "  print(\"Training script executed successfully.\")\n",
    "\n",
    "```\n",
    "\n",
    "**`src/evaluate.py`**\n",
    "\n",
    "```Py\n",
    "\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import yaml\n",
    "import os\n",
    "import json\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    roc_curve,\n",
    "    precision_recall_curve\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from mlflow.models import infer_signature\n",
    "from dotenv import load_dotenv\n",
    "from urllib.parse import urlparse\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# MLflow and DagsHub configuration\n",
    "tracking_uri = os.getenv('MLFLOW_TRACKING_URI')\n",
    "experiment_name = \"Evaluating the Trained Model Diabetes\"\n",
    "username = os.getenv('MLFLOW_TRACKING_USERNAME')\n",
    "password = os.getenv('MLFLOW_TRACKING_PASSWORD')\n",
    "\n",
    "# Set environment variables for MLflow authentication\n",
    "os.environ[\"MLFLOW_TRACKING_USERNAME\"] = username if username else \"\"\n",
    "os.environ[\"MLFLOW_TRACKING_PASSWORD\"] = password if password else \"\"\n",
    "\n",
    "print(f\"Username {username}\")\n",
    "print(f\"Username {password}\")\n",
    "\n",
    "def load_evaluation_params():\n",
    "  \"\"\"Load evaluation parameters from params.yaml\"\"\"\n",
    "  try:\n",
    "    with open('params.yaml', 'r') as file:\n",
    "      params = yaml.safe_load(file)\n",
    "    return params\n",
    "  except FileNotFoundError:\n",
    "    print(\"Warning: params.yaml not found. Using default parameters.\")\n",
    "    return {\n",
    "      'train': {\n",
    "        'input': 'dataset/processed/diabetes_processed.csv',\n",
    "        'output': 'model/diabetes_model.pkl',\n",
    "        'test_size': 0.2,\n",
    "        'random_state': 42,\n",
    "        'model_name': 'RandomForestClassifierBestModel'\n",
    "      }\n",
    "    }\n",
    "\n",
    "def load_data_and_model(data_path, model_path):\n",
    "    \"\"\"Load dataset and trained model\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"LOADING DATA AND MODEL\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    try:\n",
    "        # Load dataset\n",
    "        print(f\"Loading dataset from: {data_path}\")\n",
    "        df = pd.read_csv(data_path)\n",
    "        print(f\"âœ“ Dataset loaded successfully\")\n",
    "        print(f\"  - Dataset shape: {df.shape}\")\n",
    "        print(f\"  - Features: {df.columns.tolist()}\")\n",
    "\n",
    "        # Prepare features and target\n",
    "        X = df.drop(columns=['Outcome'])\n",
    "        y = df['Outcome']\n",
    "        print(f\"  - Features shape: {X.shape}\")\n",
    "        print(f\"  - Target shape: {y.shape}\")\n",
    "        print(f\"  - Target distribution: {y.value_counts().to_dict()}\")\n",
    "\n",
    "        # Load trained model\n",
    "        print(f\"\\nLoading trained model from: {model_path}\")\n",
    "        with open(model_path, 'rb') as f:\n",
    "            model = pickle.load(f)\n",
    "        print(f\"âœ“ Model loaded successfully\")\n",
    "        print(f\"  - Model type: {type(model).__name__}\")\n",
    "\n",
    "        return df, X, y, model\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"âœ— Error: File not found - {e}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"âœ— Error loading data or model: {e}\")\n",
    "        raise\n",
    "\n",
    "def split_data(X, y, test_size=0.2, random_state=42):\n",
    "    \"\"\"Split data into train and test sets\"\"\"\n",
    "    print(f\"\\nSplitting data:\")\n",
    "    print(f\"  - Test size: {test_size}\")\n",
    "    print(f\"  - Random state: {random_state}\")\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "\n",
    "    print(f\"âœ“ Data split completed\")\n",
    "    print(f\"  - Training set: {X_train.shape[0]} samples\")\n",
    "    print(f\"  - Test set: {X_test.shape[0]} samples\")\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def calculate_comprehensive_metrics(y_true, y_pred, y_pred_proba=None):\n",
    "    \"\"\"Calculate comprehensive evaluation metrics\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"CALCULATING EVALUATION METRICS\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    metrics = {}\n",
    "\n",
    "    # Basic metrics\n",
    "    metrics['accuracy'] = accuracy_score(y_true, y_pred)\n",
    "    metrics['precision'] = precision_score(y_true, y_pred, average='binary')\n",
    "    metrics['recall'] = recall_score(y_true, y_pred, average='binary')\n",
    "    metrics['f1_score'] = f1_score(y_true, y_pred, average='binary')\n",
    "\n",
    "    # AUC-ROC if probabilities are available\n",
    "    if y_pred_proba is not None:\n",
    "        metrics['roc_auc'] = roc_auc_score(y_true, y_pred_proba[:, 1])\n",
    "\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    metrics['confusion_matrix'] = cm.tolist()\n",
    "\n",
    "    # True/False Positives/Negatives\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    metrics['true_negatives'] = int(tn)\n",
    "    metrics['false_positives'] = int(fp)\n",
    "    metrics['false_negatives'] = int(fn)\n",
    "    metrics['true_positives'] = int(tp)\n",
    "\n",
    "    # Specificity and Sensitivity\n",
    "    metrics['specificity'] = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    metrics['sensitivity'] = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "\n",
    "    # Print metrics\n",
    "    print(f\"Basic Metrics:\")\n",
    "    print(f\"  - Accuracy: {metrics['accuracy']:.4f}\")\n",
    "    print(f\"  - Precision: {metrics['precision']:.4f}\")\n",
    "    print(f\"  - Recall (Sensitivity): {metrics['recall']:.4f}\")\n",
    "    print(f\"  - F1-Score: {metrics['f1_score']:.4f}\")\n",
    "    print(f\"  - Specificity: {metrics['specificity']:.4f}\")\n",
    "\n",
    "    if 'roc_auc' in metrics:\n",
    "        print(f\"  - ROC-AUC: {metrics['roc_auc']:.4f}\")\n",
    "\n",
    "    print(f\"\\nConfusion Matrix:\")\n",
    "    print(f\"  - True Negatives: {metrics['true_negatives']}\")\n",
    "    print(f\"  - False Positives: {metrics['false_positives']}\")\n",
    "    print(f\"  - False Negatives: {metrics['false_negatives']}\")\n",
    "    print(f\"  - True Positives: {metrics['true_positives']}\")\n",
    "\n",
    "    return metrics\n",
    "\n",
    "def generate_classification_report(y_true, y_pred):\n",
    "    \"\"\"Generate detailed classification report\"\"\"\n",
    "    print(f\"\\nDetailed Classification Report:\")\n",
    "    report = classification_report(y_true, y_pred, output_dict=True)\n",
    "    report_str = classification_report(y_true, y_pred)\n",
    "    print(report_str)\n",
    "\n",
    "    return report, report_str\n",
    "\n",
    "def create_visualizations(y_true, y_pred, y_pred_proba=None, save_plots=True):\n",
    "    \"\"\"Create evaluation visualizations\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"GENERATING VISUALIZATIONS\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    plots_created = []\n",
    "\n",
    "    try:\n",
    "        # Set style\n",
    "        plt.style.use('default')\n",
    "        sns.set_palette(\"husl\")\n",
    "\n",
    "        # 1. Confusion Matrix Heatmap\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                   xticklabels=['No Diabetes', 'Diabetes'],\n",
    "                   yticklabels=['No Diabetes', 'Diabetes'])\n",
    "        plt.title('Confusion Matrix - Diabetes Prediction')\n",
    "        plt.ylabel('True Label')\n",
    "        plt.xlabel('Predicted Label')\n",
    "\n",
    "        if save_plots:\n",
    "            confusion_matrix_path = 'artifacts/confusion_matrix.png'\n",
    "            os.makedirs('artifacts', exist_ok=True)\n",
    "            plt.savefig(confusion_matrix_path, dpi=300, bbox_inches='tight')\n",
    "            plots_created.append(confusion_matrix_path)\n",
    "            print(f\"âœ“ Confusion matrix saved to: {confusion_matrix_path}\")\n",
    "\n",
    "        plt.close()\n",
    "\n",
    "        # 2. ROC Curve (if probabilities available)\n",
    "        if y_pred_proba is not None:\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            fpr, tpr, _ = roc_curve(y_true, y_pred_proba[:, 1])\n",
    "            auc_score = roc_auc_score(y_true, y_pred_proba[:, 1])\n",
    "\n",
    "            plt.plot(fpr, tpr, linewidth=2, label=f'ROC Curve (AUC = {auc_score:.3f})')\n",
    "            plt.plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random Classifier')\n",
    "            plt.xlabel('False Positive Rate')\n",
    "            plt.ylabel('True Positive Rate')\n",
    "            plt.title('ROC Curve - Diabetes Prediction')\n",
    "            plt.legend()\n",
    "            plt.grid(True, alpha=0.3)\n",
    "\n",
    "            if save_plots:\n",
    "                roc_curve_path = 'artifacts/roc_curve.png'\n",
    "                plt.savefig(roc_curve_path, dpi=300, bbox_inches='tight')\n",
    "                plots_created.append(roc_curve_path)\n",
    "                print(f\"âœ“ ROC curve saved to: {roc_curve_path}\")\n",
    "\n",
    "            plt.close()\n",
    "\n",
    "            # 3. Precision-Recall Curve\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            precision, recall, _ = precision_recall_curve(y_true, y_pred_proba[:, 1])\n",
    "\n",
    "            plt.plot(recall, precision, linewidth=2, label='Precision-Recall Curve')\n",
    "            plt.xlabel('Recall')\n",
    "            plt.ylabel('Precision')\n",
    "            plt.title('Precision-Recall Curve - Diabetes Prediction')\n",
    "            plt.legend()\n",
    "            plt.grid(True, alpha=0.3)\n",
    "\n",
    "            if save_plots:\n",
    "                pr_curve_path = 'artifacts/precision_recall_curve.png'\n",
    "                plt.savefig(pr_curve_path, dpi=300, bbox_inches='tight')\n",
    "                plots_created.append(pr_curve_path)\n",
    "                print(f\"âœ“ Precision-Recall curve saved to: {pr_curve_path}\")\n",
    "\n",
    "            plt.close()\n",
    "\n",
    "        # 4. Prediction Distribution\n",
    "        plt.figure(figsize=(10, 6))\n",
    "\n",
    "        # Create subplots for actual vs predicted distributions\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "        # Actual distribution\n",
    "        y_true_counts = pd.Series(y_true).value_counts().sort_index()\n",
    "        ax1.bar(['No Diabetes', 'Diabetes'], y_true_counts.values,\n",
    "                color=['lightblue', 'lightcoral'], alpha=0.7)\n",
    "        ax1.set_title('Actual Distribution')\n",
    "        ax1.set_ylabel('Count')\n",
    "\n",
    "        # Predicted distribution\n",
    "        y_pred_counts = pd.Series(y_pred).value_counts().sort_index()\n",
    "        ax2.bar(['No Diabetes', 'Diabetes'], y_pred_counts.values,\n",
    "                color=['lightblue', 'lightcoral'], alpha=0.7)\n",
    "        ax2.set_title('Predicted Distribution')\n",
    "        ax2.set_ylabel('Count')\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "        if save_plots:\n",
    "            distribution_path = 'artifacts/prediction_distribution.png'\n",
    "            plt.savefig(distribution_path, dpi=300, bbox_inches='tight')\n",
    "            plots_created.append(distribution_path)\n",
    "            print(f\"âœ“ Prediction distribution saved to: {distribution_path}\")\n",
    "\n",
    "        plt.close()\n",
    "\n",
    "        print(f\"âœ“ All visualizations created successfully\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âœ— Error creating visualizations: {e}\")\n",
    "\n",
    "    return plots_created\n",
    "\n",
    "\n",
    "def log_to_mlflow(metrics, report, report_str, model, X_test, y_test, plots_created, model_name):\n",
    "    \"\"\"Log evaluation results to MLflow with DagsHub integration\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"LOGGING TO MLFLOW & DAGSHUB\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # Configure MLflow\n",
    "    print(f\"MLflow Configuration:\")\n",
    "    print(f\"  - Tracking URI: {tracking_uri}\")\n",
    "    print(f\"  - Experiment name: {experiment_name}\")\n",
    "\n",
    "    mlflow.set_tracking_uri(tracking_uri)\n",
    "\n",
    "    # Set authentication if credentials are provided\n",
    "    if username and password:\n",
    "        print(\"  - Authentication: Enabled\")\n",
    "    else:\n",
    "        print(\"  - Authentication: No credentials provided\")\n",
    "\n",
    "    # Set experiment\n",
    "    try:\n",
    "        mlflow.set_experiment(experiment_name)\n",
    "        print(f\"âœ“ Experiment set: {experiment_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âœ— Error setting experiment: {e}\")\n",
    "        # Create experiment if it doesn't exist\n",
    "        try:\n",
    "            mlflow.create_experiment(experiment_name)\n",
    "            mlflow.set_experiment(experiment_name)\n",
    "            print(f\"âœ“ Created and set new experiment: {experiment_name}\")\n",
    "        except Exception as e2:\n",
    "            print(f\"âœ— Error creating experiment: {e2}\")\n",
    "\n",
    "    # Start MLflow run\n",
    "    with mlflow.start_run(run_name=f\"diabetes_evaluation_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"):\n",
    "        print(f\"âœ“ MLflow run started\")\n",
    "\n",
    "        # Log evaluation metrics\n",
    "        print(f\"\\nLogging metrics...\")\n",
    "        for metric_name, metric_value in metrics.items():\n",
    "            if isinstance(metric_value, (int, float)) and not isinstance(metric_value, bool):\n",
    "                mlflow.log_metric(metric_name, metric_value)\n",
    "                print(f\"  - {metric_name}: {metric_value}\")\n",
    "        print(f\"âœ“ Metrics logged successfully\")\n",
    "\n",
    "        # Log parameters\n",
    "        print(f\"\\nLogging parameters...\")\n",
    "        mlflow.log_param(\"model_type\", type(model).__name__)\n",
    "        mlflow.log_param(\"test_samples\", len(y_test))\n",
    "        mlflow.log_param(\"evaluation_date\", datetime.now().isoformat())\n",
    "        mlflow.log_param(\"model_name\", model_name)\n",
    "\n",
    "        # Log model parameters if available\n",
    "        if hasattr(model, 'get_params'):\n",
    "            model_params = model.get_params()\n",
    "            for param_name, param_value in model_params.items():\n",
    "                mlflow.log_param(f\"model_{param_name}\", param_value)\n",
    "        print(f\"âœ“ Parameters logged successfully\")\n",
    "\n",
    "        # Log artifacts\n",
    "        print(f\"\\nLogging artifacts...\")\n",
    "\n",
    "        # Log confusion matrix as text\n",
    "        cm_text = f\"Confusion Matrix:\\\\n{metrics['confusion_matrix']}\"\n",
    "        mlflow.log_text(cm_text, \"confusion_matrix.txt\")\n",
    "\n",
    "        # Log classification report\n",
    "        mlflow.log_text(report_str, \"classification_report.txt\")\n",
    "\n",
    "        # Log detailed metrics as JSON\n",
    "        metrics_json = json.dumps(metrics, indent=2, default=str)\n",
    "        mlflow.log_text(metrics_json, \"detailed_metrics.json\")\n",
    "\n",
    "        # Log classification report as JSON\n",
    "        report_json = json.dumps(report, indent=2, default=str)\n",
    "        mlflow.log_text(report_json, \"classification_report.json\")\n",
    "\n",
    "        print(f\"âœ“ Text artifacts logged successfully\")\n",
    "\n",
    "        # Log visualization plots\n",
    "        for plot_path in plots_created:\n",
    "            try:\n",
    "                mlflow.log_artifact(plot_path)\n",
    "                print(f\"  - Logged plot: {plot_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"  âœ— Error logging plot {plot_path}: {e}\")\n",
    "\n",
    "        print(f\"âœ“ All artifacts logged successfully\")\n",
    "\n",
    "        # Get run info\n",
    "        run_info = mlflow.active_run().info\n",
    "        print(f\"\\n\" + \"=\"*60)\n",
    "        print(\"MLFLOW RUN SUMMARY\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Run ID: {run_info.run_id}\")\n",
    "        print(f\"Experiment ID: {run_info.experiment_id}\")\n",
    "        print(f\"Status: {run_info.status}\")\n",
    "        print(f\"Start Time: {datetime.fromtimestamp(run_info.start_time/1000)}\")\n",
    "\n",
    "        if tracking_uri and \"dagshub.com\" in tracking_uri:\n",
    "            print(f\"\\nðŸ”— View on DagsHub: {tracking_uri.replace('databricks', 'dagshub.com')}\")\n",
    "\n",
    "        return run_info.run_id\n",
    "\n",
    "\n",
    "def evaluate_model(params_path='params.yaml'):\n",
    "    \"\"\"Main evaluation function\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"DIABETES PREDICTION MODEL EVALUATION\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "    try:\n",
    "        # Load parameters\n",
    "        params = load_evaluation_params()\n",
    "        train_params = params.get('train', {})\n",
    "\n",
    "        data_path = train_params.get('input', 'dataset/processed/diabetes_processed.csv')\n",
    "        model_path = train_params.get('output', 'model/diabetes_model.pkl')\n",
    "        test_size = train_params.get('test_size', 0.2)\n",
    "        random_state = train_params.get('random_state', 42)\n",
    "        model_name = train_params.get('model_name', 'RandomForestClassifierBestModel')\n",
    "\n",
    "        print(f\"Configuration:\")\n",
    "        print(f\"  - Data path: {data_path}\")\n",
    "        print(f\"  - Model path: {model_path}\")\n",
    "        print(f\"  - Test size: {test_size}\")\n",
    "        print(f\"  - Random state: {random_state}\")\n",
    "        print(f\"  - Model name: {model_name}\")\n",
    "\n",
    "        # Load data and model\n",
    "        df, X, y, model = load_data_and_model(data_path, model_path)\n",
    "\n",
    "        # Split data (using same split as training)\n",
    "        X_train, X_test, y_train, y_test = split_data(X, y, test_size, random_state)\n",
    "\n",
    "        # Make predictions\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"MAKING PREDICTIONS\")\n",
    "        print(\"=\"*60)\n",
    "\n",
    "        y_pred = model.predict(X_test)\n",
    "        print(f\"âœ“ Predictions generated for {len(y_test)} samples\")\n",
    "\n",
    "        # Get prediction probabilities if available\n",
    "        y_pred_proba = None\n",
    "        if hasattr(model, 'predict_proba'):\n",
    "            y_pred_proba = model.predict_proba(X_test)\n",
    "            print(f\"âœ“ Prediction probabilities generated\")\n",
    "\n",
    "        # Calculate metrics\n",
    "        metrics = calculate_comprehensive_metrics(y_test, y_pred, y_pred_proba)\n",
    "\n",
    "        # Generate classification report\n",
    "        report, report_str = generate_classification_report(y_test, y_pred)\n",
    "\n",
    "        # Create visualizations\n",
    "        plots_created = create_visualizations(y_test, y_pred, y_pred_proba)\n",
    "\n",
    "        # Log to MLflow\n",
    "        run_id = log_to_mlflow(metrics, report, report_str, model, X_test, y_test, plots_created, model_name)\n",
    "\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"EVALUATION COMPLETED SUCCESSFULLY\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"Final Results Summary:\")\n",
    "        print(f\"  - Accuracy: {metrics['accuracy']:.4f}\")\n",
    "        print(f\"  - Precision: {metrics['precision']:.4f}\")\n",
    "        print(f\"  - Recall: {metrics['recall']:.4f}\")\n",
    "        print(f\"  - F1-Score: {metrics['f1_score']:.4f}\")\n",
    "        if 'roc_auc' in metrics:\n",
    "            print(f\"  - ROC-AUC: {metrics['roc_auc']:.4f}\")\n",
    "        print(f\"  - MLflow Run ID: {run_id}\")\n",
    "        print(f\"Completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "        return metrics, run_id\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nâœ— EVALUATION FAILED: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting Diabetes Prediction Model Evaluation...\")\n",
    "    metrics, run_id = evaluate_model()\n",
    "    print(\"Evaluation script executed successfully.\")\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75855cf8",
   "metadata": {},
   "source": [
    "## **Complete Pipeline**\n",
    "\n",
    "Till now we've implemented individually but this we will build the complete workflow.\n",
    "\n",
    "We will track the dataset versions i.e. `data, params.yml`\n",
    "\n",
    "First we will track the `raw data` with DVC.\n",
    "\n",
    "`dvc add dataset/raw/diabetes.csv`\n",
    "\n",
    "`git add dataset/raw/diabetes.csv.dvc`\n",
    "\n",
    "Now, to combine `preprocessing`, `evaluate` and `train` we can use amazing feature of `DVC` i.e. `DVC Stage`.\n",
    "\n",
    "### **DVC Stage**\n",
    "\n",
    "This command is used to define stages in a ML or data pipeline. These stages represent steps lika data preprocessing, model training and evaluating.\n",
    "\n",
    "```Text\n",
    "\n",
    "Preprocessing --> Training --> Evaluation\n",
    "\n",
    "```\n",
    "\n",
    "So we can use `dvc stage add` to define the stages.\n",
    "\n",
    "With `stage` we can define the `pipeline` of our project that needs to be executing the task in a sequence.\n",
    "\n",
    "**Preprocessing Stage**\n",
    "\n",
    "```bash\n",
    "\n",
    "dvc stage add -n preprocess \\\n",
    "  -p preprocess.input,preprocess.output \\\n",
    "  -d src/preprocess.py -d dataset/raw/diabetes.csv \\\n",
    "  -o dataset/processed/diabetes_processed.csv \\\n",
    "  -- python src/preprocess.py\n",
    "\n",
    "```\n",
    "\n",
    "`add -n` : Create a new stage, name of the process\n",
    "`-p` : Tracks the parameter available in the `yaml` file. As our file has `preprocess` under which we've `input` and `output`\n",
    "`-d` : Specifies the dependencies.\n",
    "`-o` : Output of the stage.\n",
    "\n",
    "`python src/preprocess.py` : The command to run the stage and the script to run.\n",
    "\n",
    "As soon as we run this command, it will create a `dvc.yaml` file in the root directory of the project. This file contains the information about the stages and their dependencies.\n",
    "The `dvc.yaml` file will look like this:\n",
    "\n",
    "```yaml\n",
    "stages:\n",
    "  preprocess:\n",
    "    cmd: python src/preprocess.py\n",
    "    deps:\n",
    "      - src/preprocess.py\n",
    "      - dataset/raw/diabetes.csv\n",
    "    outs:\n",
    "      - dataset/processed/diabetes_processed\n",
    "    params:\n",
    "      - preprocess.input\n",
    "      - preprocess.output\n",
    "```\n",
    "\n",
    "**Training Stage**\n",
    "\n",
    "```bash\n",
    "dvc stage add -n train \\\n",
    "  -p train.input, train.output, train.test_size, train.random_state, train.model_name \\\n",
    "  -d src/train.py -d dataset/processed/diabetes_processed.csv \\\n",
    "  -o model/diabetes_model.pkl \\\n",
    "  -- python src/train.py\n",
    "\n",
    "```\n",
    "\n",
    "**Evaluation Stage**\n",
    "\n",
    "```bash\n",
    "dvc stage add -n evaluate \\\n",
    "  -p evaluate.input, evaluate.output, evaluate.metric \\\n",
    "  -d src/evaluate.py -d model/diabetes_model.pkl -d dataset/processed/diabetes_processed.csv \\\n",
    "  -o reports/diabetes_report.json \\\n",
    "  python src/evaluate.py\n",
    "\n",
    "```\n",
    "\n",
    "**Viewing the Pipeline**\n",
    "To view the pipeline, we can use the `dvc pipeline show` command. This command will show the stages and their dependencies in a graphical format.\n",
    "\n",
    "```bash\n",
    "dvc pipeline show --ascii\n",
    "\n",
    "```\n",
    "\n",
    "This command will show the pipeline in a ASCII format.\n",
    "\n",
    "```Text\n",
    "Preprocessing --> Training --> Evaluation\n",
    "\n",
    "```\n",
    "\n",
    "**Running the Pipeline**\n",
    "\n",
    "```bash\n",
    "dvc repro\n",
    "```\n",
    "\n",
    "This command will run the pipeline and execute all the stages in the defined order.\n",
    "\n",
    "**Tracking the Pipeline**\n",
    "To track the pipeline, we can use `dvc dag` command. This command will show the directed acyclic graph (DAG) of the pipeline.\n",
    "\n",
    "Also, track the data version with DVC with Dagshub. We will need to add the remote repository to DVC.\n",
    "\n",
    "```bash\n",
    "dvc remote add -d origin https://dagshub.com/username/repo.git\n",
    "```\n",
    "\n",
    "Replace `username` and `repo` with your Dagshub username and repository name.\n",
    "\n",
    "Then, setup the credentials for the remote repository.\n",
    "\n",
    "```bash\n",
    "dvc remote modify origin --local access_key_id bee4f040f555191f8c8fb05458249bed6f421f06\n",
    "dvc remote modify origin --local secret_access_key bee4f040f555191f8c8fb05458249bed6f421f06\n",
    "```\n",
    "\n",
    "Now, we can push the data to the remote repository.\n",
    "\n",
    "```bash\n",
    "dvc push -r origin\n",
    "```\n",
    "\n",
    "```bash\n",
    "dvc pull -r origin\n",
    "```\n",
    "\n",
    "This command will pull the data from the remote repository and update the local repository with the latest data.\n",
    "\n",
    "Now we will need to track the `dvc.yaml` file and the `params.yaml` file in the git repository.\n",
    "\n",
    "```bash\n",
    "git add dvc.yaml params.yaml\n",
    "git commit -m \"Added DVC stages for preprocessing, training, and evaluation\"\n",
    "```\n",
    "\n",
    "Now, we can push the changes to the remote repository.\n",
    "\n",
    "```bash\n",
    "git push origin master\n",
    "```\n",
    "\n",
    "Once pushed, you can view the pipeline on Dagshub. You can also view the data versions and the parameters used in the pipeline.\n",
    "\n",
    "**But in our current implementation the DVC is not available in the Dagshub. So we will need to use the local DVC for now.**\n",
    "\n",
    "## **Conclusion**\n",
    "\n",
    "In this project, we have implemented an end-to-end machine learning pipeline using DVC and MLflow. We have tracked the data, parameters, and stages of the pipeline using DVC. We have also used MLflow to track the experiments and log the results.\n",
    "This project can be extended further by adding more stages to the pipeline, such as hyperparameter tuning, model deployment, and monitoring. DVC and MLflow provide a powerful set of tools to manage the machine learning lifecycle and make it easier to collaborate with other team members.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a202db5",
   "metadata": {},
   "source": [
    "## **MLFlow in AWS (Try It)**\n",
    "\n",
    "To run MLflow in AWS, you can use the following steps:\n",
    "\n",
    "1. **Set up an EC2 instance**: Launch an EC2 instance with the desired specifications. Make sure to configure the security group to allow inbound traffic on the port you will use for MLflow (default is 5000).\n",
    "2. **Install MLflow**: SSH into your EC2 instance and install MLflow using pip:\n",
    "   ```bash\n",
    "   pip install mlflow\n",
    "   ```\n",
    "3. **Run MLflow server**: Start the MLflow server on your EC2 instance:\n",
    "\n",
    "   ```bash\n",
    "   mlflow ui\n",
    "   ```\n",
    "\n",
    "4. **Access MLflow UI**: Open your web browser and navigate to `http://<your-ec2-public-ip>:5000` to access the MLflow UI.\n",
    "\n",
    "5. **Configure tracking URI**: In your MLflow scripts, set the tracking URI to point to your EC2 instance:\n",
    "   ```python\n",
    "   import mlflow\n",
    "   mlflow.set_tracking_uri(\"http://<your-ec2-public-ip>:5000\")\n",
    "   ```\n",
    "6. **Log experiments**: Use MLflow's logging functions in your scripts to log parameters, metrics, and models as you would normally do.\n",
    "7. **Persist data**: If you want to persist your MLflow data, you can set up a remote storage backend (like S3) for MLflow artifacts and models. You can configure this in your `mlflow.set_tracking_uri()` call or by setting environment variables.\n",
    "8. **Security**: Consider securing your MLflow server by setting up authentication and HTTPS. You can use tools like Nginx or Apache to set up a reverse proxy with SSL.\n",
    "9. **Monitoring and Scaling**: For production use, consider using AWS services like CloudWatch for monitoring and Auto Scaling for handling increased load.\n",
    "\n",
    "```bash\n",
    "10. **Backup**: Regularly back up your MLflow data and models to ensure you don't lose any important information.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8459649d",
   "metadata": {},
   "source": [
    "## **Docker**\n",
    "\n",
    "### **Dockerfile**\n",
    "\n",
    "```Dockerfile\n",
    "# Use the official Python image as the base image\n",
    "FROM python:3.9-slim\n",
    "# Set the working directory in the container\n",
    "WORKDIR /app\n",
    "# Copy the requirements file into the container\n",
    "COPY requirements.txt .\n",
    "# Install the required packages\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "# Copy the entire application code into the container\n",
    "COPY . .\n",
    "# Expose the port that MLflow will run on\n",
    "EXPOSE 5000\n",
    "# Set environment variables for MLflow\n",
    "ENV MLFLOW_TRACKING_URI=http://localhost:5000\n",
    "ENV MLFLOW_EXPERIMENT_NAME=Diabetes_Prediction_Experiment\n",
    "# Command to run the MLflow server\n",
    "CMD [\"mlflow\", \"ui\", \"--host\", \"0.0.0.0\"]\n",
    "```\n",
    "\n",
    "### _Docker Compose_\n",
    "\n",
    "```yaml\n",
    "version: \"3.8\"\n",
    "services:\n",
    "  mlflow:\n",
    "    build: .\n",
    "    ports:\n",
    "      - \"5000:5000\"\n",
    "    environment:\n",
    "      MLFLOW_TRACKING_URI: http://localhost:5000\n",
    "      MLFLOW_EXPERIMENT_NAME: Diabetes_Prediction_Experiment\n",
    "    volumes:\n",
    "      - ./mlruns:/app/mlruns\n",
    "```\n",
    "\n",
    "### **Build and Run the Docker Container**\n",
    "\n",
    "```bash\n",
    "# Build the Docker image\n",
    "docker build -t mlflow-diabetes-prediction .\n",
    "# Run the Docker container\n",
    "docker run -p 5000:5000 mlflow-diabetes-prediction\n",
    "```\n",
    "\n",
    "### **Docker Volume**\n",
    "\n",
    "**What is Docker Volume?**\n",
    "\n",
    "Docker volumes are a way to persist data generated by and used by Docker containers. When you create a volume, it is stored outside the container's filesystem, allowing you to keep the data even if the container is stopped or removed. This is particularly useful for applications like MLflow, where you want to retain experiment logs, models, and artifacts across container restarts.\n",
    "\n",
    "To use a Docker volume with MLflow, you can modify the `docker-compose.yml` file to include a volume for the `mlruns` directory, which is where MLflow stores its experiment data.\n",
    "\n",
    "```yaml\n",
    "version: \"3.8\"\n",
    "services:\n",
    "  mlflow:\n",
    "    build: .\n",
    "    ports:\n",
    "      - \"5000:5000\"\n",
    "    environment:\n",
    "      MLFLOW_TRACKING_URI: http://localhost:5000\n",
    "      MLFLOW_EXPERIMENT_NAME: Diabetes_Prediction_Experiment\n",
    "    volumes:\n",
    "      - ./mlruns:/app/mlruns # This line mounts the local mlruns directory to the container\n",
    "```\n",
    "\n",
    "This configuration mounts the `mlruns` directory from your local machine to the `/app/mlruns` directory in the container. This way, any experiments, models, and artifacts logged by MLflow will be stored in the `mlruns` directory on your host machine, allowing you to access them even after stopping or removing the container.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d42d031",
   "metadata": {},
   "source": [
    "## **Apache Airflow**\n",
    "\n",
    "**Intro**\n",
    "\n",
    "Apache Airflow is an open-source platform to programmatically author, schedule, and monitor workflows. It allows you to define complex data pipelines as Directed Acyclic Graphs (DAGs) using Python code. Airflow provides a rich user interface to visualize the execution of tasks, monitor their status, and manage dependencies between tasks.\n",
    "\n",
    "**Installation**\n",
    "\n",
    "To install Apache Airflow, you can use the following command:\n",
    "\n",
    "```bash\n",
    "pip install apache-airflow\n",
    "```\n",
    "\n",
    "**Basic Concepts**\n",
    "\n",
    "1. **DAG (Directed Acyclic Graph)**: A DAG is a collection of tasks with dependencies defined between them. In Airflow, you define your workflows as DAGs using Python code.\n",
    "\n",
    "2. **Operators**: Operators are the building blocks of Airflow tasks. They define what kind of work a task will do. There are different types of operators for various tasks, such as BashOperator for running bash commands, PythonOperator for executing Python functions, and more.\n",
    "\n",
    "3. **Tasks**: A task is a single unit of work within a DAG. Each task is represented by an instance of an operator.\n",
    "\n",
    "4. **Scheduler**: The Airflow scheduler is responsible for executing tasks on a defined schedule. It monitors the DAGs and triggers tasks based on their dependencies and schedules.\n",
    "\n",
    "5. **Executor**: The executor is the component that actually runs the tasks. Airflow supports different executors, such as the LocalExecutor for running tasks locally and the CeleryExecutor for distributed task execution.\n",
    "\n",
    "6. **UI**: Airflow provides a web-based user interface to visualize and manage your workflows. You can view the status of tasks, trigger runs, and monitor progress through the UI.\n",
    "\n",
    "**Example DAG**\n",
    "\n",
    "```python\n",
    "from airflow import DAG\n",
    "from airflow.operators.dummy_operator import DummyOperator\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "from datetime import datetime\n",
    "\n",
    "def my_task():\n",
    "    print(\"Hello, Airflow!\")\n",
    "\n",
    "with DAG(\"my_dag\", schedule_interval=\"@daily\", start_date=datetime(2023, 1, 1)) as dag:\n",
    "    start = DummyOperator(task_id=\"start\")\n",
    "    task1 = PythonOperator(task_id=\"task1\", python_callable=my_task)\n",
    "    end = DummyOperator(task_id=\"end\")\n",
    "\n",
    "    start >> task1 >> end\n",
    "\n",
    "```\n",
    "\n",
    "### **Running Airflow**\n",
    "\n",
    "To run Airflow, you need to initialize the database and start the web server and scheduler. Here are the steps:\n",
    "\n",
    "```bash\n",
    "# Initialize the database\n",
    "airflow db init\n",
    "# Start the web server\n",
    "airflow webserver --port 8080\n",
    "# Start the scheduler\n",
    "airflow scheduler\n",
    "```\n",
    "\n",
    "### **Accessing the Airflow UI**\n",
    "\n",
    "Once the web server is running, you can access the Airflow UI by navigating to `http://localhost:8080` in your web browser. From there, you can view your DAGs, trigger runs, and monitor task execution.\n",
    "\n",
    "### **Airflow with MLFlow**\n",
    "\n",
    "To integrate Airflow with MLflow, you can create Airflow tasks that log experiments, parameters, and metrics to MLflow. This allows you to track your machine learning experiments and manage them effectively.\n",
    "\n",
    "```python\n",
    "from airflow import DAG\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "from datetime import datetime\n",
    "import mlflow\n",
    "\n",
    "def log_experiment():\n",
    "  mlflow.start_run()\n",
    "  mlflow.log_param(\"param1\", 5)\n",
    "  mlflow.log_metric(\"metric1\", 0.85)\n",
    "  mlflow.end_run()\n",
    "\n",
    "with DAG(\"mlflow_integration\", schedule_interval=\"@daily\", start_date=datetime(2023, 1, 1)) as dag:\n",
    "  log_task = PythonOperator(task_id=\"log_experiment\", python_callable=log_experiment)\n",
    "  log_task\n",
    "\n",
    "```\n",
    "\n",
    "This DAG defines a task that logs parameters and metrics to MLflow. You can run this DAG in Airflow to track your machine learning experiments.\n",
    "\n",
    "### **Why Airflow for MLOps?**\n",
    "\n",
    "Apache Airflow is a powerful tool for managing complex workflows and data pipelines, making it an excellent choice for MLOps (Machine Learning Operations). Here are some reasons why Airflow is suitable for MLOps:\n",
    "\n",
    "1. **Workflow Orchestration**: Airflow allows you to define complex workflows as Directed Acyclic Graphs (DAGs), making it easy to manage dependencies between tasks and ensure that tasks are executed in the correct order.\n",
    "\n",
    "2. **Task Scheduling**: Airflow provides a robust scheduling mechanism that allows you to run tasks at specific intervals or based on triggers, making it ideal for automating machine learning workflows.\n",
    "\n",
    "3. **Extensibility**: Airflow supports a wide range of operators and hooks, allowing you to integrate with various tools and services commonly used in MLOps, such as MLflow, Kubernetes, and cloud platforms.\n",
    "\n",
    "4. **Monitoring and Logging**: Airflow provides a web-based user interface to monitor the status of tasks, view logs, and track the progress of workflows. This is crucial for debugging and maintaining machine learning pipelines.\n",
    "\n",
    "5. **Scalability**: Airflow can scale horizontally by using distributed executors like Celery or Kubernetes, allowing you to handle large-scale machine learning workloads efficiently.\n",
    "\n",
    "6. **Version Control**: Airflow allows you to version your DAGs and tasks, making it easier to manage changes to your machine learning workflows over time.\n",
    "\n",
    "### **Astro With Airflow**\n",
    "\n",
    "Astro is a managed service for Apache Airflow that simplifies the deployment, scaling, and management of Airflow instances. It provides a user-friendly interface and integrates with various cloud services, making it easier to run and manage Airflow workflows in production.\n",
    "\n",
    "```python\n",
    "from airflow import DAG\n",
    "from airflow.providers.docker.operators.docker import DockerOperator\n",
    "from datetime import datetime\n",
    "with DAG(\n",
    "    \"docker_example\",\n",
    "    schedule_interval=\"@daily\",\n",
    "    start_date=datetime(2023, 1, 1),\n",
    "    catchup=False,\n",
    ") as dag:\n",
    "    docker_task = DockerOperator(\n",
    "        task_id=\"run_docker_container\",\n",
    "        image=\"python:3.9-slim\",\n",
    "        api_version=\"auto\",\n",
    "        auto_remove=True,\n",
    "        command=\"python -c 'print(\\\"Hello from Docker!\\\")'\",\n",
    "        docker_url=\"unix://var/run/docker.sock\",\n",
    "        network_mode=\"bridge\",\n",
    "    )\n",
    "\n",
    "    docker_task\n",
    "```\n",
    "\n",
    "# Astro with Airflow Example\n",
    "\n",
    "This example demonstrates how to use the DockerOperator in Airflow to run a Docker container that executes a simple Python command. The DockerOperator allows you to run tasks inside Docker containers, providing isolation and reproducibility for your workflows.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e1d8f0",
   "metadata": {},
   "source": [
    "### **Next Day**\n",
    "\n",
    "**Docker Intro**\n",
    "\n",
    "https://www.udemy.com/course/complete-mlops-bootcamp-with-10-end-to-end-ml-projects/learn/lecture/45859167#overview\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
