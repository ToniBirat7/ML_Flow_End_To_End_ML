{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f15a1316",
   "metadata": {},
   "source": [
    "# **Note_2 is Continued from Note 1**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bba8f3",
   "metadata": {},
   "source": [
    "## **Intro to ETL**\n",
    "\n",
    "ETL stands for Extract, Transform, Load. It is a process used in data warehousing and data integration to move data from various sources into a centralized repository, such as a data warehouse or a database. The ETL process consists of three main steps:\n",
    "\n",
    "1. **Extract**: This step involves retrieving data from various source systems, which can include databases, flat files, APIs, or other data sources. The goal is to gather all relevant data needed for analysis.\n",
    "\n",
    "2. **Transform**: In this step, the extracted data is cleaned, transformed, and prepared for analysis. This may involve filtering, aggregating, joining, or reshaping the data to ensure it is in the right format and structure for the target system.\n",
    "\n",
    "3. **Load**: The final step is to load the transformed data into the target system, such as a data warehouse or a database. This makes the data available for querying and analysis by business intelligence tools or other applications.\n",
    "\n",
    "## **ETL Process Overview**\n",
    "\n",
    "The ETL process can be visualized as a pipeline where data flows through each of the three stages. Hereâ€™s a high-level overview of the ETL process:\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[Extract] --> B[Transform]\n",
    "    B --> C[Load]\n",
    "    C --> D[Data Warehouse]\n",
    "    D --> E[Business Intelligence Tools]\n",
    "```\n",
    "\n",
    "## **Key Components of ETL**\n",
    "\n",
    "- **Data Sources**: These can be databases, flat files, APIs, or any other systems where data resides.\n",
    "\n",
    "- **ETL Tools**: Software applications that facilitate the ETL process, such as Apache Nifi, Talend, Informatica, or custom scripts.\n",
    "\n",
    "- **Data Warehouse**: A centralized repository where transformed data is stored for analysis and reporting.\n",
    "\n",
    "- **Data Quality**: Ensuring the accuracy, consistency, and reliability of the data throughout the ETL process.\n",
    "\n",
    "- **Scheduling and Automation**: ETL processes can be scheduled to run at specific intervals or triggered by events to ensure data is always up-to-date.\n",
    "\n",
    "## **Benefits of ETL**\n",
    "\n",
    "- **Centralized Data**: ETL allows organizations to consolidate data from multiple sources into a single repository, making it easier to access and analyze.\n",
    "\n",
    "- **Improved Data Quality**: The transformation step helps clean and standardize data, improving its quality for analysis.\n",
    "\n",
    "- **Enhanced Decision Making**: By providing a unified view of data, ETL supports better decision-making processes within organizations.\n",
    "\n",
    "- **Scalability**: ETL processes can be designed to handle large volumes of data, making them suitable for growing datasets.\n",
    "\n",
    "## **ETL vs. ELT**\n",
    "\n",
    "While ETL is a traditional approach, there is also a modern variant known as ELT (Extract, Load, Transform). In ELT, data is first extracted and loaded into the target system (like a data lake or data warehouse) before the transformation occurs. This approach leverages the processing power of modern databases to perform transformations after loading, allowing for more flexibility and scalability.\n",
    "\n",
    "## **Conclusion**\n",
    "\n",
    "ETL is a crucial process in data management that enables organizations to extract valuable insights from their data by transforming and loading it into a centralized repository. Understanding the ETL process is essential for anyone involved in data warehousing, business intelligence, or data integration projects. By implementing effective ETL practices, organizations can enhance their data quality, streamline their analytics processes, and make informed decisions based on accurate and timely information.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6df3fcd",
   "metadata": {},
   "source": [
    "## **End to End ETL Pipeline with Airflow**\n",
    "\n",
    "In this section, we will explore how to implement an end-to-end ETL pipeline using Apache Airflow, a powerful open-source tool for orchestrating complex workflows. Airflow allows you to define, schedule, and monitor ETL tasks efficiently.\n",
    "\n",
    "### **Problem Statement**\n",
    "\n",
    "We will create an ETL pipeline that extracts data from a public API, transforms it by cleaning and aggregating the data, and then loads it into a PostgreSQL database. The pipeline will be scheduled to run daily.\n",
    "\n",
    "We will use the NASA API to extract data about asteroids and their close approaches to Earth. The data will be transformed to calculate the average size of asteroids and then loaded into a PostgreSQL database for further analysis.\n",
    "\n",
    "We will dockerize the Airflow environment to ensure consistency and ease of deployment. The pipeline will consist of the following steps:\n",
    "\n",
    "1. **Extract**: Fetch data from the NASA API.\n",
    "2. **Transform**: Clean the data, calculate the average size of asteroids, and prepare it for loading.\n",
    "3. **Load**: Insert the transformed data into a PostgreSQL database.\n",
    "\n",
    "Both the Airflow and PostgreSQL services will be run in Docker containers, allowing for easy setup and management of the ETL pipeline. Here we will learn how the communication between the Airflow and PostgreSQL containers is established using Docker networking.\n",
    "\n",
    "We will also implement the airflow hooks to ensure that the pipeline runs smoothly and handles any errors that may occur during the ETL process.\n",
    "\n",
    "We will use different Airflow operators to perform the ETL tasks, including the `PythonOperator` for custom Python functions, the `PostgresOperator` for executing SQL commands, and the `DockerOperator` for running tasks in Docker containers and HTTP operator to make HTTP requests to the NASA API.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1327d8",
   "metadata": {},
   "source": [
    "## **Project Begins**\n",
    "\n",
    "Refer to `Airflow_ETL_Pipeline_Astro_Postgres`\n",
    "\n",
    "Note that folder name should never contain special characters like `(), ; -` etc.\n",
    "\n",
    "As by default both the `Airflow` and `Postgres` will be running in separate docker container. Therefore, we will need to establish a communication between these two.\n",
    "\n",
    "We create `docker-compose.yml` file that will create a `Postgres` image with db_name, uname, password and env_vars.\n",
    "\n",
    "### **Docker-Compose**\n",
    "\n",
    "```yml\n",
    "version: \"3\"\n",
    "services:\n",
    "  postgres:\n",
    "    image: postgres:13\n",
    "    container_name: postgres_db\n",
    "    environment:\n",
    "      POSTGRES_USER: birat\n",
    "      POSTGRES_PASSWORD: admin\n",
    "      POSTGRES_DB: postgres\n",
    "    ports:\n",
    "      - \"5432:5432\"\n",
    "    volumes:\n",
    "      - postgres_data:/var/lib/postgresql/data\n",
    "    networks:\n",
    "      - airflow_network\n",
    "\n",
    "networks:\n",
    "  airflow_network:\n",
    "    external: false\n",
    "```\n",
    "\n",
    "In the above `yml` file, we use the `postgres` image with all the environment variables.\n",
    "\n",
    "`Volume` tracks the data for consistency and `Network`\n",
    "\n",
    "We will need to have a common network so that the containers can talk.\n",
    "\n",
    "### **ETL DAG**\n",
    "\n",
    "```Python\n",
    "\n",
    "from airflow import DAG\n",
    "from airflow.providers.http.operators.http import SimpleHttpOperator\n",
    "from airflow.operators.python import PythonOperator\n",
    "from airflow.decorators import task\n",
    "from airflow.providers.postgres.hooks.postgres import PostgresHook\n",
    "import json\n",
    "from airflow.utils.dates import days_ago\n",
    "from httpx import post\n",
    "\n",
    "# Define the DAG\n",
    "\n",
    "default_args = {\n",
    "    'owner': 'airflow',\n",
    "    'depends_on_past': False,\n",
    "    'start_date': days_ago(1),\n",
    "    'retries': 1,\n",
    "}\n",
    "\n",
    "with DAG(\n",
    "    'etl_dag',\n",
    "    default_args=default_args,\n",
    "    description='A simple ETL DAG',\n",
    "    schedule='@daily',\n",
    ") as dag:\n",
    "\n",
    "  # Step 1: Create the table if it does not exist\n",
    "\n",
    "    @task\n",
    "    def create_table():\n",
    "        pg_hook = PostgresHook(\n",
    "            postgres_conn_id='postgres_default',\n",
    "        )\n",
    "\n",
    "        # SQL command to create the table for the API\n",
    "        create_table_sql = \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS apod_data (\n",
    "            id SERIAL PRIMARY KEY,\n",
    "            title VARCHAR(255),\n",
    "            explanation TEXT,\n",
    "            url TEXT,\n",
    "            date DATE,\n",
    "            media_type VARCHAR(50),\n",
    "        );\n",
    "        \"\"\"\n",
    "\n",
    "        # Execute the SQL command\n",
    "        pg_hook.run(create_table_sql)\n",
    "  # Step 2: Extract data from the API\n",
    "\n",
    "  # Step 3: Transform the data\n",
    "\n",
    "  # Step 4: Load the data into PostgreSQL\n",
    "\n",
    "  #\n",
    "\n",
    "```\n",
    "\n",
    "In the above code, we use `PostgresHook` to interact with the `Postgres`.\n",
    "\n",
    "`pg_hook.run(create_table_sql)` to execute the `SQL Query`\n",
    "\n",
    "**Setting Up the API**\n",
    "\n",
    "Below is the format of the API :\n",
    "\n",
    "```Json\n",
    "\n",
    "  \"copyright\": \"\\nIreneusz Nowak\\n\",\n",
    "  \"date\": \"2025-05-27\",\n",
    "  \"explanation\": \"Behold one of the most photogenic regions of the night sky, captured impressively.  Featured, the band of our Milky Way Galaxy runs diagonally along the bottom-left corner, while the colorful Rho Ophiuchi cloud complex is visible just right of center and the large red circular Zeta Ophiuchi Nebula appears near the top.  In general, red emanates from nebulas glowing in the light of excited hydrogen gas, while blue marks interstellar dust preferentially reflecting the light of bright young stars.  Thick dust usually appears dark brown.  Many iconic objects of the night sky appear, including (can you find them?) the bright star Antares, the globular star cluster M4, and the Blue Horsehead nebula. This wide field composite, taken over 17 hours, was captured from South Africa last June.    Explore Your Universe: Random APOD Generator\",\n",
    "  \"hdurl\": \"https://apod.nasa.gov/apod/image/2505/RhoZeta_Nowak_2560.jpg\",\n",
    "  \"media_type\": \"image\",\n",
    "  \"service_version\": \"v1\",\n",
    "  \"title\": \"Zeta and Rho Ophiuchi with Milky Way\",\n",
    "  \"url\": \"https://apod.nasa.gov/apod/image/2505/RhoZeta_Nowak_960.jpg\"\n",
    "}\n",
    "\n",
    "```\n",
    "\n",
    "```Py\n",
    "\n",
    "  # Step 2: Extract data from the API\n",
    "    # api_endpoint = 'https://api.nasa.gov/planetary/apod?api_key=2qhSecq7ZVI2TyDgfHGAhblGmrl2Q7ZZHdj1b6Ij'\n",
    "    extract_apod = SimpleHttpOperator(\n",
    "        task_id='extract_apod',\n",
    "        http_conn_id='apod_api', # Connection ID for the NASA API\n",
    "        endpoint='planetary/apod', # Endpoint for the Astronomy Picture of the Day\n",
    "        method='GET',\n",
    "        data={\"api_key\": \"{{ conn.nasa_api.extra_dejson.api_key }}\"}, # API key from the connection\n",
    "        response_filter=lambda response: response.json(), # Filter to get JSON response\n",
    "  )\n",
    "\n",
    "```\n",
    "\n",
    "We use `SimpleHttpOperator` to send HTTP request. In this hook we provide `http_conn_id = apod_api` we will make use of `Airflow` connection in the `UI`. Then `endpoint` as to hit the `API`, the base path will be available from Airflow. Then finally `data` we get the `api_key` which is also be received from the `Airflow` connection.\n",
    "\n",
    "Then finally we want the `response_filter` in `Json`.\n",
    "\n",
    "### **Transforming the Data**\n",
    "\n",
    "```Py\n",
    "\n",
    "    @task\n",
    "    def transform_data(response):\n",
    "        apod_data = {\n",
    "            \"title\": response.get(\"title\", \"\"),\n",
    "            \"explanation\": response.get(\"explanation\", \"\"),\n",
    "            \"url\": response.get(\"url\", \"\"),\n",
    "            \"date\": response.get(\"date\", \"\"),\n",
    "            \"media_type\": response.get(\"media_type\", \"\"),\n",
    "        }\n",
    "        return apod_data\n",
    "\n",
    "```\n",
    "\n",
    "### **Loading the Data**\n",
    "\n",
    "```Py\n",
    "\n",
    "    @task\n",
    "    def load_data_to_postgres(apo_data):\n",
    "        # Initialize PostgresHook\n",
    "\n",
    "        pg_hook = PostgresHook(postgres_conn_id=\"my_postgres_connection\")\n",
    "\n",
    "        # Define SQL Query\n",
    "        insert_sql = \"\"\"\n",
    "        INSERT INTO apod_data (title, explanation, url, date, media_type)\n",
    "        VALUES (%s, %s, %s, %s, %s);\n",
    "        \"\"\"\n",
    "\n",
    "        # Execute the insert query\n",
    "        pg_hook.run(\n",
    "            insert_sql,\n",
    "            parameters=(\n",
    "                apo_data[\"title\"],\n",
    "                apo_data[\"explanation\"],\n",
    "                apo_data[\"url\"],\n",
    "                apo_data[\"date\"],\n",
    "                apo_data[\"media_type\"],\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    # Define the task dependencies\n",
    "    # Extracting the APOD data and transforming it into a format suitable for PostgreSQL\n",
    "    create_table_task = create_table() >> extract_apod\n",
    "    extract_apod_task = extract_apod.output >> transform_data\n",
    "    # Transforming the data and loading it into PostgreSQL\n",
    "    transform_data_task = transform_data(extract_apod_task)\n",
    "    # Loading the transformed data into PostgreSQL\n",
    "    load_data_to_postgres(transform_data_task)\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f55bbb0",
   "metadata": {},
   "source": [
    "**Get Inside the Astro Container** : `astro dev bash`\n",
    "\n",
    "**Start Without Cache** : `astro dev restart --no-cache`\n",
    "\n",
    "### **Depricated Things in Airflow**\n",
    "\n",
    "`SimpleHttpOperator`, `Days_Before`\n",
    "\n",
    "### **Important Steps Before Running the Astro**\n",
    "\n",
    "As astro completely runs in the multiple Docker Container, it already by default installs all the required packages such as `Airflow`, `Postgres` and other. But, it does not install any other dependcies.\n",
    "\n",
    "Therefore, we will need to include the name of the package in the `requirement.txt` of `Astro` project.\n",
    "\n",
    "For example, I was getting a lots of error because I was installing the ` apache-airflow-providers-http` package in my environment but actually it needs to be installed inside the `container`.\n",
    "\n",
    "Therefore, always add the `dependencies` inside the `requirements.txt` file.\n",
    "\n",
    "Also, the `SimpleHttpOperator` is already depricated. We need to use `HttpOperator` which is provided by the third party providers.\n",
    "\n",
    "[Providers_Package_Ref](https://airflow.apache.org/docs/apache-airflow-providers/packages-ref.html#id49)\n",
    "\n",
    "The `username` and `password` in the `docker-compose` for the postgres should be `postgres` only. If we try to keep other names we will encounter error as `postgres` would be the default admin user while creating the container.\n",
    "\n",
    "### **Setting the Important Connection (API and DB)**\n",
    "\n",
    "Now, for the program to run properly we will need to setup the two important connection i.e. `Postgres` and other is `API`.\n",
    "\n",
    "**API**\n",
    "\n",
    "Go to the `Airflow` UI search for Connection.\n",
    "\n",
    "<img src='./Notes_Img/a1.png'>\n",
    "\n",
    "<img src='./Notes_Img/a2.png'>\n",
    "\n",
    "We need to fill these `infos`, later when the `DAG` is executed the info will be reterived from here. The name should be the same.\n",
    "\n",
    "**Postgres**\n",
    "\n",
    "Click add connection.\n",
    "\n",
    "Enter the `conn_id` used in the `DAG`. When the `Astro` starts the `Postgres` it will run the `Postgres` container with the details provided in the `docker-compose.yml` file. This file contains the `username` and `password`.\n",
    "\n",
    "For the `host` go the container look for `postgres` click then copy the name of the container and paste in the `Host`.\n",
    "\n",
    "<img src='./Notes_Img/a3.png'>\n",
    "\n",
    "Then excute the `DAG` from the `Airflow` UI.\n",
    "\n",
    "## **Possible Errors While Running DAG**\n",
    "\n",
    "Due to volume inconsistency the `Username` and `Password` won't be able to find by the `Postgres` container due to which we will have to remove the `Volume` from the container.\n",
    "\n",
    "```bash\n",
    "\n",
    "(mlflow_env) toni-birat@tonibirat:/media/toni-birat/New Volume/ML_Flow_Complete/Airflow_ETL_Pipeline_Astro_Postgres$ docker volume ls\n",
    "DRIVER    VOLUME NAME\n",
    "local     4d2e2ee6bd280a5cb2f26d998389a0fd0aef5270ce1d2814d9c22617e27b6d02\n",
    "local     59e773b558bc859c8819420ab883efbbd80ba0f820be1ef86ad10981f870f657\n",
    "local     airflow-etl-pipeline-astro-postgres_54d206_airflow_logs\n",
    "local     airflow-etl-pipeline-astro-postgres_54d206_postgres_data\n",
    "local     airflow-practice_d1986e_airflow_logs\n",
    "local     airflow-practice_d1986e_postgres_data\n",
    "local     mariadb_data\n",
    "(mlflow_env) toni-birat@tonibirat:/media/toni-birat/New Volume/ML_Flow_Complete/Airflow_ETL_Pipeline_Astro_Postgres$ docker volume rm airflow_etl_pipeline_astro_postgres_postgres_data\n",
    "Error response from daemon: get airflow_etl_pipeline_astro_postgres_postgres_data: no such volume\n",
    "(mlflow_env) toni-birat@tonibirat:/media/toni-birat/New Volume/ML_Flow_Complete/Airflow_ETL_Pipeline_Astro_Postgres$ docker volume rm airflow_etl_pipeline_astro_postgres_postgres_data\n",
    "Error response from daemon: get airflow_etl_pipeline_astro_postgres_postgres_data: no such volume\n",
    "(mlflow_env) toni-birat@tonibirat:/media/toni-birat/New Volume/ML_Flow_Complete/Airflow_ETL_Pipeline_Astro_Postgres$ docker volume rm airflow-etl-pipeline-astro-postgres_54d206_airflow_logs\n",
    "airflow-etl-pipeline-astro-postgres_54d206_airflow_logs\n",
    "(mlflow_env) toni-birat@tonibirat:/media/toni-birat/New Volume/ML_Flow_Complete/Airflow_ETL_Pipeline_Astro_Postgres$ docker volume rm airflow-etl-pipeline-astro-postgres_54d206_postgres_data\n",
    "airflow-etl-pipeline-astro-postgres_54d206_postgres_data\n",
    "(mlflow_env) toni-birat@tonibirat:/media/toni-birat/New Volume/ML_Flow_Complete/Airflow_ETL_Pipeline_Astro_Postgres$ docker volume rm airflow-practice_d1986e_postgres_data\n",
    "airflow-practice_d1986e_postgres_data\n",
    "(mlflow_env) toni-birat@tonibirat:/media/toni-birat/New Volume/ML_Flow_Complete/Airflow_ETL_Pipeline_Astro_Postgres$\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1452e1",
   "metadata": {},
   "source": [
    "Once all the DAGs are completed we've successfully implement or automated the complete `ETL` pipeline project.\n",
    "\n",
    "As we've `Postgres` container we can't directly viewe the tables and our rows. For that we will need to install `dbeaver community`\n",
    "\n",
    "<img src='./Notes_Img/a4.png'>\n",
    "\n",
    "**DBeaver**\n",
    "\n",
    "<img src='./Notes_Img/a5.png'>\n",
    "\n",
    "Go to the Database, look for `apod_data`\n",
    "\n",
    "<img src='./Notes_Img/a6.png'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bea15f5",
   "metadata": {},
   "source": [
    "### **Advantage of Providing the Connection Variables from Airflow UI**\n",
    "\n",
    "We can pass any creds, remote host id, api keys.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f04dfc",
   "metadata": {},
   "source": [
    "## **Deploying the Astro Project in the Astro Cloud and AWS**\n",
    "\n",
    "[Video_Link](https://www.udemy.com/course/complete-mlops-bootcamp-with-10-end-to-end-ml-projects/learn/lecture/46199315#overview)\n",
    "\n",
    "**Note**\n",
    "\n",
    "There was problem with Deployment in the `Astro Cloud`. Fix it any other Day.\n",
    "\n",
    "```Bash\n",
    "\n",
    "(mlflow_env) toni-birat@tonibirat:/media/toni-birat/New Volume/ML_Flow_Complete/Airflow_ETL_Pipeline_Astro_Postgres$ astro deploy\n",
    "Authenticated to Astro\n",
    "\n",
    "Error: This command is not yet supported on Airflow 3 deployments\n",
    "\n",
    "```\n",
    "\n",
    "**Astro.io**\n",
    "\n",
    "We will host the airflow application in the Astro Cloud. Login, Create Account.\n",
    "\n",
    "Create the organization. Name the project.\n",
    "\n",
    "**AWS Database**\n",
    "\n",
    "Start the `RDS` and create database.\n",
    "\n",
    "Create a posgrest.\n",
    "\n",
    "Set the rules.\n",
    "\n",
    "Copy the end point address.\n",
    "\n",
    "<hr>\n",
    "\n",
    "We will use `Astro CLI` for the deployment.\n",
    "\n",
    "`astro login`\n",
    "\n",
    "`astro deploy` : Choose the project.\n",
    "\n",
    "Once deployed go to `DAGs` in `Astro UI`\n",
    "\n",
    "Open the `Airflow` UI and then set the connections. Once, the connection is set run the DAGs.\n",
    "\n",
    "Now, you can visualize the remote database as well using `DBeuer` past the host location from `AWS`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b4dbf7",
   "metadata": {},
   "source": [
    "## **Project Status Update - May 28, 2025**\n",
    "\n",
    "### **âœ… Completed Successfully:**\n",
    "\n",
    "1. **Local Development Environment:**\n",
    "\n",
    "   - Astro project setup complete\n",
    "   - Docker containers running properly\n",
    "   - Dependencies installed (HTTP & PostgreSQL providers)\n",
    "   - ETL DAG created and functional\n",
    "\n",
    "2. **ETL Pipeline Components:**\n",
    "\n",
    "   - âœ… Extract: NASA APOD API integration\n",
    "   - âœ… Transform: Data cleaning and structuring\n",
    "   - âœ… Load: PostgreSQL database insertion\n",
    "   - âœ… Scheduling: Daily execution configured\n",
    "\n",
    "3. **Infrastructure:**\n",
    "   - âœ… Docker containerization\n",
    "   - âœ… PostgreSQL database setup\n",
    "   - âœ… Airflow connections configuration\n",
    "   - âœ… Volume management and networking\n",
    "\n",
    "### **ðŸ”§ Current Challenge:**\n",
    "\n",
    "**Deployment Issue:** Airflow 3.0 runtime not supported by `astro deploy` command\n",
    "\n",
    "### **ðŸ“‹ Next Steps:**\n",
    "\n",
    "#### **Immediate Actions:**\n",
    "\n",
    "1. **Runtime Downgrade (Recommended):**\n",
    "\n",
    "   ```bash\n",
    "   # Update Dockerfile to use Airflow 2.x\n",
    "   FROM astrocrpublic.azurecr.io/runtime:2.9.0\n",
    "   ```\n",
    "\n",
    "2. **Test Deployment:**\n",
    "\n",
    "   ```bash\n",
    "   astro dev stop\n",
    "   astro dev start --no-cache\n",
    "   astro deploy\n",
    "   ```\n",
    "\n",
    "3. **AWS RDS Setup:**\n",
    "   - Create PostgreSQL instance\n",
    "   - Configure security groups\n",
    "   - Update Airflow connections\n",
    "\n",
    "#### **Alternative Approaches:**\n",
    "\n",
    "1. **GitHub Actions CI/CD Pipeline:**\n",
    "\n",
    "   - Automated deployment\n",
    "   - Version control integration\n",
    "   - Production-ready workflow\n",
    "\n",
    "2. **Manual Docker Deployment:**\n",
    "   - Container registry push\n",
    "   - Kubernetes/ECS deployment\n",
    "   - Custom orchestration\n",
    "\n",
    "#### **Future Enhancements:**\n",
    "\n",
    "1. **Pipeline Improvements:**\n",
    "\n",
    "   - Error handling and retry logic\n",
    "   - Data validation and quality checks\n",
    "   - Monitoring and alerting\n",
    "   - Multiple data sources integration\n",
    "\n",
    "2. **Infrastructure Scaling:**\n",
    "\n",
    "   - Load balancing\n",
    "   - Auto-scaling configurations\n",
    "   - Multi-environment setup (dev/staging/prod)\n",
    "\n",
    "3. **Advanced Features:**\n",
    "   - Real-time data streaming\n",
    "   - Machine learning integration\n",
    "   - Dashboard and visualization\n",
    "   - Data lineage tracking\n",
    "\n",
    "### **ðŸŽ¯ Learning Objectives Achieved:**\n",
    "\n",
    "- âœ… End-to-end ETL pipeline design\n",
    "- âœ… Apache Airflow workflow orchestration\n",
    "- âœ… Docker containerization for data applications\n",
    "- âœ… API integration and error handling\n",
    "- âœ… Database design for analytical workloads\n",
    "- âœ… Production deployment considerations\n",
    "\n",
    "### **ðŸ“š Key Takeaways:**\n",
    "\n",
    "1. **Version Compatibility:** Always check runtime compatibility before deployment\n",
    "2. **Local Testing:** Ensure thorough local testing before cloud deployment\n",
    "3. **Dependency Management:** Proper requirements.txt configuration is crucial\n",
    "4. **Connection Management:** Centralized connection configuration in Airflow UI\n",
    "5. **Volume Management:** Proper Docker volume handling for data persistence\n",
    "\n",
    "**Project successfully demonstrates modern data engineering practices and provides a solid foundation for production-ready ETL pipelines.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68d26f5",
   "metadata": {},
   "source": [
    "## **Github Action and CI CD Pipeline**\n",
    "\n",
    "In this section, we will explore how to set up a CI/CD pipeline using GitHub Actions for our Airflow ETL project. This will allow us to automate the deployment of our Airflow DAGs whenever we push changes to our GitHub repository.\n",
    "\n",
    "It allows us to create custom workflows that can be triggered by various events, such as pushing code to a repository, creating a pull request, or scheduling a workflow to run at specific intervals.\n",
    "\n",
    "### **Continuous Integration and Continuous Deployment (CI/CD)**\n",
    "\n",
    "CI/CD is a software development practice that automates the process of integrating code changes, testing them, and deploying them to production. It helps ensure that code changes are reliable, consistent, and can be deployed quickly.\n",
    "\n",
    "**Continuous Integration (CI)**: This involves automatically building and testing code changes whenever they are pushed to a repository. It helps catch bugs early in the development process. With GitHub Actions, you can define workflows that run tests when code is pushed or pull requests are created.\n",
    "\n",
    "With this many developers working on the same project, it is essential to have a system that can automatically test and deploy code changes to ensure that the project remains stable and functional. If some tests fail, the CI process will notify the developers, allowing them to fix issues before merging code into the main branch.\n",
    "\n",
    "**Continuous Deployment (CD)**: This involves automatically deploying code changes to production after they have passed the CI tests. It ensures that the latest code is always available in production without manual intervention. Github Actions can be configured to deploy applications to various environments, such as staging or production, based on the success of the CI tests. This practice reduces the time between code changes and their deployment, allowing for faster delivery of new features and bug fixes.\n",
    "\n",
    "### **Setting Up GitHub Actions for Airflow ETL Project**\n",
    "\n",
    "To set up GitHub Actions for our Airflow ETL project, we will create a workflow file that defines the steps to be executed whenever changes are pushed to the repository. This workflow will include steps to:\n",
    "\n",
    "1. **Check out the code**: Use the `actions/checkout` action to check out the code from the repository.\n",
    "2. **Set up Python**: Use the `actions/setup-python` action to set up the Python environment with the required version.\n",
    "3. **Install dependencies**: Use `pip` to install the required Python packages, including Airflow and any other dependencies specified in the `requirements.txt` file.\n",
    "4. **Run tests**: Execute any tests defined in the project to ensure that the code is functioning correctly.\n",
    "5. **Deploy to Airflow**: Use the `astro deploy` command to deploy the Airflow DAGs to the Astro Cloud or any other Airflow environment.\n",
    "\n",
    "### **Creating the Developers Workflow File**\n",
    "\n",
    "A developer workflow file is a `YAML file` that defines the steps to be executed in a GitHub Actions workflow. It specifies the events that trigger the workflow, the jobs to be run, and the individual steps within each job.\n",
    "\n",
    "To create the workflow file, we will create a directory called `.github/workflows` in the root of our repository and add a file named `ci-cd.yml`. This file will define the workflow for our Airflow ETL project.\n",
    "\n",
    "```yaml\n",
    "name: CI/CD Pipeline for Airflow ETL Project\n",
    "on:\n",
    "  push:\n",
    "    branches:\n",
    "      - main\n",
    "  pull_request:\n",
    "    branches:\n",
    "      - main\n",
    "jobs:\n",
    "  build:\n",
    "    runs-on: ubuntu-latest\n",
    "    steps:\n",
    "      - name: Check out code\n",
    "        uses: actions/checkout@v2\n",
    "\n",
    "      - name: Set up Python\n",
    "        uses: actions/setup-python@v2\n",
    "        with:\n",
    "          python-version: \"3.8\"\n",
    "\n",
    "      - name: Install dependencies\n",
    "        run: |\n",
    "          python -m pip install --upgrade pip\n",
    "          pip install -r requirements.txt\n",
    "\n",
    "      - name: Run tests\n",
    "        run: |\n",
    "          # Add your test command here, e.g., pytest or unittest\n",
    "          echo \"Running tests...\"\n",
    "\n",
    "      - name: Deploy to Airflow\n",
    "        run: |\n",
    "          astro deploy --project-id <your_project_id>\n",
    "```\n",
    "\n",
    "### **Explanation of the Workflow File**\n",
    "\n",
    "- **name**: The name of the workflow, which will be displayed in the GitHub Actions UI.\n",
    "\n",
    "- **on**: Specifies the events that will trigger the workflow. In this case, it will run on pushes and pull requests to the `main` branch.\n",
    "\n",
    "- **jobs**: Defines the jobs that will be executed in the workflow. Each job runs in a separate environment.\n",
    "\n",
    "- **runs-on**: Specifies the type of virtual machine to use for the job. In this case, we are using the latest version of Ubuntu.\n",
    "\n",
    "- **steps**: Defines the individual steps to be executed in the job.\n",
    "\n",
    "  - **Check out code**: Uses the `actions/checkout` action to check out the code from the repository.\n",
    "\n",
    "  - **Set up Python**: Uses the `actions/setup-python` action to set up the Python environment with the specified version.\n",
    "\n",
    "  - **Install dependencies**: Installs the required Python packages using `pip`.\n",
    "\n",
    "  - **Run tests**: Placeholder for running tests. You can replace this with your actual test command.\n",
    "\n",
    "  - **Deploy to Airflow**: Uses the `astro deploy` command to deploy the Airflow DAGs to the Astro Cloud or any other Airflow environment.\n",
    "\n",
    "### **Configuring Secrets for Deployment**\n",
    "\n",
    "To securely store sensitive information such as API keys, database credentials, or deployment tokens, you can use GitHub Secrets. These secrets can be accessed in your workflow file without exposing them in the code.\n",
    "\n",
    "To add secrets to your GitHub repository:\n",
    "\n",
    "1. Go to your repository on GitHub.\n",
    "\n",
    "2. Click on \"Settings\" in the top menu.\n",
    "\n",
    "3. In the left sidebar, click on \"Secrets and variables\" and then \"Actions\".\n",
    "\n",
    "4. Click on \"New repository secret\" to add a new secret.\n",
    "\n",
    "5. Enter a name for the secret (e.g, `ASTRO_DEPLOY_TOKEN`) and its value (e.g, your Astro deployment token).\n",
    "\n",
    "6. Click \"Add secret\" to save it.\n",
    "\n",
    "You can then access these secrets in your workflow file using the `secrets` context. For example, to use the `ASTRO_DEPLOY_TOKEN` secret, you can modify the deploy step as follows:\n",
    "\n",
    "```yaml\n",
    "- name: Deploy to Airflow\n",
    "  run: |\n",
    "    astro deploy --project-id <your_project_id> --token ${{ secrets.ASTRO_DEPLOY_TOKEN }} --environment production\n",
    "```\n",
    "\n",
    "### **Running the Workflow**\n",
    "\n",
    "Once you have created the workflow file and committed it to your repository, GitHub Actions will automatically trigger the workflow whenever changes are pushed to the `main` branch or a pull request is created.\n",
    "\n",
    "You can monitor the progress of the workflow in the \"Actions\" tab of your GitHub repository. If any step fails, you can view the logs to diagnose and fix the issue.\n",
    "\n",
    "### **Conclusion**\n",
    "\n",
    "When the Automated CI Pipeline is set up, it will automatically run the defined steps whenever changes are pushed to the repository. This ensures that your Airflow ETL project is always tested and if passes the tests it will be merged to the main branch.\n",
    "\n",
    "Upon merging, CD is triggered, and the latest code is deoplyed in the Production environment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8fab158",
   "metadata": {},
   "source": [
    "## **First Github Action Project**\n",
    "\n",
    "Refer to `First_Github_Action_Project`\n",
    "\n",
    "We will create a simple GitHub Action that runs a Python script to print \"Hello, World!\" whenever code is pushed to the repository.\n",
    "\n",
    "For this, we will create a new repository on GitHub and set up a workflow file to define the action. This repo is the sub module of the main repo.\n",
    "\n",
    "[Link_Repo](https://github.com/ToniBirat7/Github_Action_Project)\n",
    "\n",
    "```Bash\n",
    "\n",
    "(mlflow_env) toni-birat@tonibirat:/media/toni-birat/New Volume/ML_Flow_Complete/First_Github_Action_Project$ git submodule add git@github.com:ToniBirat7/Github_Action_Project.git First_Github_Action_Project\n",
    "\n",
    "git add .gitmodules First_Github_Action_Project\n",
    "\n",
    "(mlflow_env) toni-birat@tonibirat:/media/toni-birat/New Volume/ML_Flow_Complete/First_Github_Action_Project$ git commit -m \"Added First Github Action Project as submodule\"\n",
    "\n",
    "```\n",
    "\n",
    "We will use `pandas` and `pytest` for this project. We will create a simple Python script that prints \"Hello, World!\" and a test to verify its functionality.\n",
    "\n",
    "Below is the structure of the project:\n",
    "\n",
    "```\n",
    "First_Github_Action_Project/\n",
    "â”œâ”€â”€ .github/\n",
    "â”‚   â””â”€â”€ workflows/\n",
    "â”‚       â”œâ”€â”€ ci.yml                 # Main CI/CD pipeline\n",
    "â”‚       â”œâ”€â”€ code-quality.yml       # Code quality checks\n",
    "â”‚       â””â”€â”€ release.yml            # Release automation\n",
    "â”œâ”€â”€ src/\n",
    "â”‚   â”œâ”€â”€ __init__.py\n",
    "â”‚   â”œâ”€â”€ math_operations.py         # Core Python functions\n",
    "â”œâ”€â”€ tests/\n",
    "â”‚   â”œâ”€â”€ __init__.py\n",
    "â”‚   â”œâ”€â”€ test_math_operations.py    # Unit tests\n",
    "â”œâ”€â”€ requirements.txt               # Python dependencies\n",
    "â”œâ”€â”€ requirements-dev.txt           # Development dependencies\n",
    "â”œâ”€â”€ .gitignore                     # Git ignore rules\n",
    "â”œâ”€â”€ Dockerfile                     # Container configuration\n",
    "â”œâ”€â”€ docker-compose.yml             # Multi-service setup\n",
    "â””â”€â”€ README.md                      # This file\n",
    "```\n",
    "\n",
    "Once we push the code to the repository, we will need to use `Actions` to create the workflow file. So, first let's choose the `Actions` tab in the GitHub repository.\n",
    "\n",
    "Select and `configure` the `Python application` template. This will create a basic workflow file for us.\n",
    "\n",
    "We will need to create a a workflow file in the `.github/workflows` directory of our repository. This file will define the steps to be executed whenever code is pushed to the repository or a pull request is created.\n",
    "\n",
    "The workflow file will be created in the `.github/workflows` directory. The default name is `python-app.yml`, but we can rename it to `unit_test.yml` for clarity.\n",
    "\n",
    "It is better to copy the template of the `python-app.yml` workflow file and paste it in the `unit_test.yml` file.\n",
    "\n",
    "We write workflow in the Key Value pair format. The key is the name of the workflow and the value is the steps to be executed.\n",
    "\n",
    "```yaml\n",
    "# Name: Python application\n",
    "name: Python application\n",
    "\n",
    "# Triggers the workflow on push or pull request events to the main branch\n",
    "on:\n",
    "  push:\n",
    "    branches:\n",
    "      - main\n",
    "  pull_request:\n",
    "    branches:\n",
    "      - main\n",
    "\n",
    "# Action to run the workflow\n",
    "jobs:\n",
    "  # Name of the job\n",
    "  build:\n",
    "    runs-on: ubuntu-latest # The job will run on the latest version of Ubuntu\n",
    "\n",
    "    steps:\n",
    "      - name: Checkout code\n",
    "        # This step checks out the repository so that the workflow can access its contents\n",
    "        uses: actions/checkout@v4\n",
    "\n",
    "      # Set up Python environment\n",
    "      # This step sets up Python 3.10 for the workflow\n",
    "      - name: Set up Python 3.10\n",
    "        uses: actions/setup-python@v3\n",
    "        with:\n",
    "          python-version: \"3.10\"\n",
    "\n",
    "      # Install dependencies and run linting and tests\n",
    "      # This step installs the necessary Python packages and runs linting and tests\n",
    "      - name: Install dependencies\n",
    "        run: |\n",
    "          python -m pip install --upgrade pip\n",
    "          if [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n",
    "\n",
    "      # Run the tests\n",
    "      - name: Run tests\n",
    "        run: pytest\n",
    "```\n",
    "\n",
    "This workflow file defines the following steps:\n",
    "\n",
    "1. **Triggers**: The workflow is triggered on pushes and pull requests to the `main` branch.\n",
    "\n",
    "2. **Jobs**: The workflow consists of a single job named `build`, which runs on the latest version of Ubuntu.\n",
    "\n",
    "3. **Steps**:\n",
    "   - **Checkout code**: Uses the `actions/checkout` action to check out the code from the repository.\n",
    "   - **Set up Python**: Uses the `actions/setup-python` action to set up Python 3.10 for the workflow.\n",
    "   - **Install dependencies**: Installs the required Python packages from `requirements.txt`.\n",
    "   - **Run tests**: Executes the tests using `pytest`.\n",
    "\n",
    "`pytest` by default looks for files in the `tests` directory that start with `test_` or end with `_test.py`. It will automatically discover and run all the test functions defined in those files.\n",
    "\n",
    "Next, step is to copy the yml file and paste it in the `.github/workflows` directory of our repository.\n",
    "\n",
    "We will need to push the changes to the repository.\n",
    "\n",
    "Then we will commit the workflow file in the `GithHub` UI. As soon as we commit the file, the workflow will be triggered and the tests will be executed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758d0904",
   "metadata": {},
   "source": [
    "## **End to End Github Action Workflow with DockerHub**\n",
    "\n",
    "In this section, we will extend our GitHub Actions workflow to include Docker integration. This will allow us to build a Docker image for our Python application and push it to Docker Hub whenever changes are pushed to the repository.\n",
    "\n",
    "### **Project Description**\n",
    "\n",
    "We will have a simple Flask App, for which we will have `Unit Tests` and `Integration Tests`.\n",
    "\n",
    "After each push, we will build a Docker image, run the tests, and push the image to Docker Hub if the tests pass. This ensures that our application is always in a deployable state.\n",
    "\n",
    "Whenever we are using `CD` we will need to have a `Secret` in the `GitHub` repository that contains the `Docker Hub` credentials. This will allow us to authenticate and push the Docker image to our Docker Hub account.\n",
    "\n",
    "`Secrets` includes `Docker Hub Username` and `Docker Hub Password`. We will need to add these secrets in the `GitHub` repository settings.\n",
    "\n",
    "### **Setting Up the Project Structure**\n",
    "\n",
    "```plaintext\n",
    "End_to_End_Github_Action_Workflow/\n",
    "â”œâ”€â”€ .github/\n",
    "â”‚   â””â”€â”€ workflows/\n",
    "â”‚       â””â”€â”€ docker-ci-cd.yml        # GitHub Actions workflow\n",
    "â”œâ”€â”€ app.py\n",
    "â”œâ”€â”€ Dockerfile                      # Dockerfile for building the image\n",
    "â”œâ”€â”€ requirements.txt                # Python dependencies\n",
    "â”œâ”€â”€ tests/\n",
    "â”‚   â”œâ”€â”€ __init__.py\n",
    "â”‚   â”œâ”€â”€ test_app.py                 # Unit tests for the Flask\n",
    "```\n",
    "\n",
    "First we'll create a simple Flask application in the `app.py` file. This application will have a single endpoint that returns \"Hello, World!\".\n",
    "\n",
    "```python\n",
    "\n",
    "from flask import Flask\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/')\n",
    "def hello_world():\n",
    "    return 'Hello, World! This is a Flask app running in a Docker container.'\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(host='0.0.0.0', port=5000)\n",
    "\n",
    "```\n",
    "\n",
    "Next, we will create a `requirements.txt` file to specify the dependencies for our Flask application. This file will include Flask and any other required packages.\n",
    "\n",
    "Then we'll write the test cases for our Flask application in the `tests/test_app.py` file. We'll use `pytest` for testing.\n",
    "\n",
    "Then we will need to create a `Dockerfile` to build the Docker image for our Flask application. The Dockerfile will define the base image, copy the application code, install dependencies, and expose the necessary port.\n",
    "\n",
    "```dockerfile\n",
    "\n",
    "FROM python:3.8-slim\n",
    "\n",
    "WORKDIR /app\n",
    "\n",
    "COPY . /app/\n",
    "\n",
    "RUN pip install -r requirements.txt\n",
    "\n",
    "EXPOSE 5000\n",
    "\n",
    "CMD [\"python\", \"app.py\"]\n",
    "\n",
    "```\n",
    "\n",
    "This Dockerfile does the following:\n",
    "\n",
    "1. **FROM python:3.8-slim**: Uses the official Python 3.8 slim image as the base image.\n",
    "\n",
    "2. **WORKDIR /app**: Sets the working directory to `/app` inside the container.\n",
    "\n",
    "3. **COPY . /app/**: Copies the entire application code into the `/app` directory inside the container.\n",
    "\n",
    "4. **RUN pip install -r requirements.txt**: Installs the Python dependencies specified in the `requirements.txt` file.\n",
    "\n",
    "5. **EXPOSE 5000**: Exposes port 5000, which is the default port for Flask applications.\n",
    "\n",
    "6. **CMD [\"python\", \"app.py\"]**: Specifies the command to run the application when the container starts.\n",
    "\n",
    "Now we will write the CI/CD workflow file in the `.github/workflows` directory. This file will define the steps to build the Docker image, run tests, and push the image to Docker Hub.\n",
    "\n",
    "```yaml\n",
    "name: CI/CD for Dockerized Flask App\n",
    "\n",
    "on:\n",
    "  push:\n",
    "    branches: [\"main\"]\n",
    "  pull_request:\n",
    "    branches: [\"main\"]\n",
    "\n",
    "jobs:\n",
    "  build-and-test:\n",
    "    runs-on: ubuntu-latest\n",
    "\n",
    "    steps:\n",
    "      - name: Checkout code\n",
    "        # This step checks out the repository so that the workflow can access its contents\n",
    "        uses: actions/checkout@v4\n",
    "\n",
    "      - name: Set Up Python\n",
    "        uses: actions/setup-python@v4\n",
    "        with:\n",
    "          python-version: \"3.10\"\n",
    "\n",
    "      - name: Install Dependencies\n",
    "        run: |\n",
    "          python -m pip install --upgrade pip\n",
    "          if [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n",
    "\n",
    "      - name: Run Tests\n",
    "        run: pytest\n",
    "\n",
    "    # Build and push Docker image\n",
    "  build-and-push:\n",
    "    runs-on: ubuntu-latest\n",
    "    needs: build-and-test\n",
    "\n",
    "    steps:\n",
    "      - name: Checkout code\n",
    "        # This step checks out the repository so that the workflow can access its contents\n",
    "        uses: actions/checkout@v4\n",
    "\n",
    "      - name: Set up Docker Buildx\n",
    "        # This step sets up Docker Buildx, which is a Docker CLI plugin for extended build capabilities with BuildKit\n",
    "        uses: docker/setup-buildx-action@v2\n",
    "\n",
    "      - name: Log in to Docker Hub\n",
    "        # This step logs in to Docker Hub using the credentials stored in GitHub Secrets\n",
    "        uses: docker/login-action@v2\n",
    "        with:\n",
    "          # Replace with your Docker Hub username and password stored in GitHub Secrets\n",
    "          username: ${{ secrets.DOCKER_USERNAME }}\n",
    "          password: ${{ secrets.DOCKER_PASSWORD }}\n",
    "\n",
    "      # Build and push Docker image\n",
    "      - name: Build Docker image\n",
    "        uses: docker/build-push-action@v4\n",
    "        with:\n",
    "          context: .\n",
    "          push: true\n",
    "          tags: ${{ secrets.DOCKER_USERNAME }}/flask-app:latest\n",
    "\n",
    "      - name: Image digest\n",
    "        run: echo ${{ steps.build-and-push.outputs.digest }}\n",
    "```\n",
    "\n",
    "### **Explanation of the Workflow File**\n",
    "\n",
    "- **name**: The name of the workflow, which will be displayed in the GitHub Actions UI.\n",
    "\n",
    "- **on**: Specifies the events that will trigger the workflow. In this case, it will run on pushes and pull requests to the `main` branch.\n",
    "\n",
    "- **jobs**: Defines the jobs that will be executed in the workflow. Each job runs in a separate environment.\n",
    "\n",
    "- **build-and-test**: This job runs on the latest version of Ubuntu and performs the following steps:\n",
    "\n",
    "  - **Checkout code**: Uses the `actions/checkout` action to check out the code from the repository.\n",
    "  - **Set Up Python**: Uses the `actions/setup-python` action to set up Python 3.10 for the workflow.\n",
    "  - **Install Dependencies**: Installs the required Python packages from `requirements.txt`.\n",
    "  - **Run Tests**: Executes the tests using `pytest`.\n",
    "\n",
    "- **build-and-push**: This job runs after the `build-and-test` job and performs the following steps:\n",
    "\n",
    "  - **Checkout code**: Uses the `actions/checkout` action to check out the code from the repository.\n",
    "  - **Set up Docker Buildx**: Sets up Docker Buildx, which is a Docker CLI plugin for extended build capabilities with BuildKit.\n",
    "  - **Log in to Docker Hub**: Uses the `docker/login-action` action to log in to Docker Hub using the credentials stored in GitHub Secrets. You will need to create these secrets in your GitHub repository settings.\n",
    "  - **Build Docker image**: Uses the `docker/build-push-action` action to build and push the Docker image to Docker Hub. The image will be tagged with the username and `flask-app:latest`.\n",
    "  - **Image digest**: Prints the image digest after building and pushing the image.\n",
    "\n",
    "**Accessing the Docker Hub Username and Password**\n",
    "\n",
    "To get the Username and Password from the Docker Hub, we will need to create a Personal Access Token (PAT) in Docker Hub. This token will be used to authenticate with Docker Hub when pushing the image.\n",
    "\n",
    "Then copy the token and paste it in the GitHub repository `Secrets and Variables` under `action` under `Repo Secrets` add the `DOCKER_HUB_PASSWORD`. The username will be your Docker Hub username, which you can also add as a secret named `DOCKER_HUB_USERNAME`.\n",
    "\n",
    "Now, the GitHub Actions workflow can access these secrets to authenticate with Docker Hub.\n",
    "\n",
    "Once all the tests pass, the Docker image will be built and pushed to Docker Hub automatically.\n",
    "\n",
    "Now, try to pull the Docker image from Docker Hub using the following command:\n",
    "\n",
    "```bash\n",
    "docker pull <your-dockerhub-username>/flask-app:latest\n",
    "```\n",
    "\n",
    "## **Special Notes**\n",
    "\n",
    "The name of the Dockerfile should be `Dockerfile` and it should be in the root directory of the repository. The GitHub Actions workflow will look for this file to build the Docker image. Because the `docker/build-push-action` action uses the `context` parameter to specify the build context, which is the root directory of the repository in this case.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf94b789",
   "metadata": {},
   "source": [
    "# **First Complete End to End Project with Airflow, Postgres, and GitHub Actions**\n",
    "\n",
    "<br>\n",
    "\n",
    "Refer to `First_Complete_End_to_End_ML_Project`.\n",
    "\n",
    "[Github](git@github.com:ToniBirat7/First_Complete_End_to_End_ML_Project.git)\n",
    "\n",
    "<br>\n",
    "\n",
    "In this project, we will create a complete production-ready end-to-end machine learning project using Apache Airflow, PostgreSQL, and GitHub Actions.\n",
    "\n",
    "We will learn how to write production ready code, set up a CI/CD pipeline, and deploy our machine learning model using Airflow and PostgreSQL.\n",
    "\n",
    "**Creating Project Structure**\n",
    "\n",
    "In real world projects, we will have a specific project structure that we will follow. This will help us to organize our code and make it easier to maintain.\n",
    "\n",
    "For that, we will create a `template.py` file that will contain the project structure. We run this file to create the project structure.\n",
    "\n",
    "```python\n",
    "\n",
    "# Automated Script for a complete end-to-end ML project\n",
    "\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import os\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "project_name = \"First_Complete_End_To_End_ML_Project\"\n",
    "\n",
    "list_of_files = [\n",
    "  \".github/workflows/.gitkeep\",\n",
    "  f\"src/{project_name}/__init__.py\",\n",
    "  f\"src/{project_name}/components/__init__.py\",\n",
    "  f\"src/{project_name}/components/data_ingestion.py\",\n",
    "  f\"src/{project_name}/components/data_validation.py\",\n",
    "  f\"src/{project_name}/components/data_transformation.py\",\n",
    "  f\"src/{project_name}/components/model_trainer.py\",\n",
    "  f\"src/{project_name}/components/model_evaluation.py\",\n",
    "  f\"src/{project_name}/components/model_pusher.py\",\n",
    "  f\"src/{project_name}/utils/__init__.py\",\n",
    "  f\"src/{project_name}/utils/common.py\",\n",
    "  f\"src/{project_name}/utils/logger.py\",\n",
    "  f\"src/{project_name}/config/__init__.py\",\n",
    "  f\"src/{project_name}/config/configuration.py\",\n",
    "  f\"src/{project_name}/pipeline/__init__.py\",\n",
    "  f\"src/{project_name}/entity/__init__.py\",\n",
    "  f\"src/{project_name}/entity/config_entity.py\",\n",
    "  f\"src/{project_name}/constants/__init__.py\",\n",
    "  \"config/config.yaml\",\n",
    "  \"params.yaml\",\n",
    "  \"schema.yaml\",\n",
    "  \"main.py\",\n",
    "  \"Dockerfile\",\n",
    "  \"setup.py\",\n",
    "  \"research/research.ipynb\",\n",
    "  \"templates/index.html\"\n",
    "]\n",
    "\n",
    "for filepath in list_of_files:\n",
    "    filepath = Path(filepath)\n",
    "    filedir,filename = os.path.split(filepath)\n",
    "    print(f\"Processing file: {filedir}, {filename}\")\n",
    "    print(f\"Type of file: {type(filedir)}\")\n",
    "    if not os.path.exists(filedir):\n",
    "        if (not filedir):\n",
    "            logging.info(f\"File directory is empty, creating in current directory.\")\n",
    "        else:\n",
    "          logging.info(f\"Creating directory: {filedir}\")\n",
    "          os.makedirs(filedir,exist_ok=True)\n",
    "    if (not os.path.exists(filepath)) or (os.path.getsize(filepath) == 0):\n",
    "        logging.info(f\"Creating file: {filepath}\")\n",
    "        with open(filepath, 'w') as f:\n",
    "          logging.info(f\"Writing to file: {filepath}\")\n",
    "          # Write a comment or placeholder content based on the file type\n",
    "          if filename == \"__init__.py\":\n",
    "              f.write(\"# This is an init file for the package\\n\")\n",
    "          if filename == \"config.yaml\":\n",
    "              f.write(\"# Configuration file for the project\\n\")\n",
    "          elif filename == \"params.yaml\":\n",
    "              f.write(\"# Parameters for the project\\n\")\n",
    "          elif filename == \"schema.yaml\":\n",
    "              f.write(\"# Schema for the project\\n\")\n",
    "          elif filename == \"main.py\":\n",
    "              f.write(\"# Main entry point for the project\\n\")\n",
    "          elif filename == \"Dockerfile\":\n",
    "              f.write(\"# Dockerfile for the project\\n\")\n",
    "          elif filename == \"setup.py\":\n",
    "              f.write(\"# Setup script for the project\\n\")\n",
    "          else:\n",
    "              f.write(f\"# {filename} file\\n\")\n",
    "    else:\n",
    "        logging.info(f\"File already exists and is not empty: {filepath}\")\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f3fa17",
   "metadata": {},
   "source": [
    "### **Loggin Implementation and Exception Handling**\n",
    "\n",
    "In this section, we will implement logging in our project. Logging is an essential part of any production-ready application. It helps us to track the flow of the application, debug issues, and monitor the performance of the application.\n",
    "\n",
    "Now, we will implement logging in our project. We will create log for each `package`. We will need to write the logging code in the `__init__.py` file of each package. Whenever we import the package, the logging code will be executed and the log file will be created. We will import the initialied `logger` object in the `__init__.py` file of each package.\n",
    "\n",
    "```python\n",
    "\n",
    "# This is an init file for the package\n",
    "# __init__.py file\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "logging_format = \"[%(asctime)s] - %(levelname)s - %(message)s\"\n",
    "\n",
    "log_dirs = 'logs'\n",
    "log_filepath = os.path.join(log_dirs, 'logging.log')\n",
    "\n",
    "if not os.path.exists(log_dirs):\n",
    "    os.makedirs(log_dirs)\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=logging_format,\n",
    "\n",
    "    handlers=[\n",
    "        logging.FileHandler(log_filepath), # Log to a file\n",
    "        logging.StreamHandler(sys.stdout) # Log to console\n",
    "    ]\n",
    ")\n",
    "\n",
    "src_logger = logging.getLogger(\"First_Complete_End_To_End_ML_Project\") # Create a logger for the package\n",
    "\n",
    "```\n",
    "\n",
    "Then try to import the package in the `main.py` file. This will execute the logging code and create the log file. Every log will be written to the log file and also printed to the console.\n",
    "\n",
    "```python\n",
    "\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b694f1",
   "metadata": {},
   "source": [
    "## **Setting Up the Utility Functions and Exception Handling**\n",
    "\n",
    "**Utility Functions**\n",
    "\n",
    "Utility functions are reusable functions that can be used across different parts of the project. We will create a `utils` package and add the utility functions in it.\n",
    "\n",
    "In this section, we will set up the utility functions for our project. Utility functions are reusable functions that can be used across different parts of the project. We will create a `utils` package and add the utility functions in it.\n",
    "\n",
    "There's `common.py` file in the `utils` package. We will add the utility functions in this file.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba6aef1",
   "metadata": {},
   "source": [
    "## **Config Box and Annotations**\n",
    "\n",
    "In this section, we will set up the configuration for our project using `ConfigBox`. `ConfigBox` is a Python library that allows us to create a configuration object that can be easily accessed and modified.\n",
    "\n",
    "```Python\n",
    "example = {\n",
    "  \"key1\": \"value1\",\n",
    "  \"key2\": \"value2\",\n",
    "  \"key3\": \"value3\",\n",
    "}\n",
    "\n",
    "# To access the value of \"key1\" no error\n",
    "example[\"key3\"]\n",
    "```\n",
    "\n",
    "```Python\n",
    "# But to everytime access a key we've to use square brackets\n",
    "# Instead to make it more readable we can use . just like accessing attributes of an object\n",
    "# But for that we will need to bind the dictionary with ConfigBox\n",
    "\n",
    "# Gives Error\n",
    "\n",
    "example.key1\n",
    "```\n",
    "\n",
    "```Python\n",
    "\n",
    "# Import ConfigBox\n",
    "\n",
    "from box import ConfigBox\n",
    "\n",
    "# Bind the dictionary with ConfigBox\n",
    "\n",
    "example = ConfigBox(example)\n",
    "\n",
    "# Now we can access the value of \"key1\" using dot notation\n",
    "\n",
    "example.key1\n",
    "\n",
    "# Output: value1\n",
    "```\n",
    "\n",
    "**Why ConfigBox is Useful?**\n",
    "\n",
    "We've many `yaml` files in our project, and we need to load them into our module. By default `yaml` files are loaded as dictionaries, which can be cumbersome to work with. `ConfigBox` provides a structured way to handle configurations, making it easier to manage and validate them.\n",
    "\n",
    "For example, if you have a configuration file with various settings, you can define a schema using ConfigBox annotations to ensure that the data adheres to specific types and constraints. This helps catch errors early and provides a clear structure for your configuration data.\n",
    "\n",
    "### **Ensure Annotations**\n",
    "\n",
    "To ensure that the annotations are correctly applied to the configuration data, you can use the `ensure_annotations` function from the `ensure` module. This function checks if the configuration data matches the defined schema and raises an error if there are any discrepancies.\n",
    "\n",
    "Let's say we've a function to multiply two numbers, and we want to ensure that the inputs are integers. We'll need to defime the schema while defining the function, and then use `ensure_annotations` to validate the inputs.\n",
    "\n",
    "`ensure_annotations` is a decorator that can be applied to functions to enforce type annotations at runtime. It checks the types of the arguments (schema that is defined for the parameters) and the return value against the specified annotations.\n",
    "\n",
    "It will raise a `TypeError` if the types do not match the annotations, ensuring that the function is called with the correct types.\n",
    "\n",
    "```python\n",
    "\n",
    "# Without ensure_annotation\n",
    "\n",
    "def add(a: int, b: int ):\n",
    "    return a + b\n",
    "\n",
    "# If we pass a string to the function, it will not raise an error immediately\n",
    "result = add(\"1\", \"2\")\n",
    "result\n",
    "\n",
    "# Output: \"12\" (string concatenation)\n",
    "\n",
    "# Our function will return \"12\" instead of 3\n",
    "\n",
    "# To ensure that the types are correct, we can use ConfigBox with ensure_annotation\n",
    "```\n",
    "\n",
    "```python\n",
    "\n",
    "from box import ConfigBox\n",
    "from ensure import ensure_annotations\n",
    "\n",
    "@ensure_annotations\n",
    "def add(a: int, b: int ):\n",
    "    return a + b\n",
    "\n",
    "# If we pass a string to the function, it will raise an error\n",
    "\n",
    "add(\"1\", \"2\")\n",
    "\n",
    "# This will raise an error: TypeError: Expected int, got str instead\n",
    "\n",
    "```\n",
    "\n",
    "### **ConfigBox with Ensure Annotations**\n",
    "\n",
    "```Python\n",
    "\n",
    "# We can implement the combination of ConfigBox and ensure_annotations in our code\n",
    "\n",
    "# To fix this, we can use ConfigBox to ensure that the types are correct\n",
    "example = ConfigBox({\"a\": 1, \"b\": 2})\n",
    "\n",
    "@ensure_annotations\n",
    "def add(a: int, b: int ) -> int:\n",
    "    \"\"\"\n",
    "    Function to add two numbers.\n",
    "    Args:\n",
    "        a (int): First number.\n",
    "        b (int): Second number.\n",
    "    Returns:\n",
    "        int: Sum of a and b.\n",
    "    \"\"\"\n",
    "    return a + b\n",
    "# Now we can pass the ConfigBox object to the function\n",
    "result = add(example.a, example.b)\n",
    "\n",
    "result\n",
    "\n",
    "# Output: 3\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb7c1b2",
   "metadata": {},
   "source": [
    "## **Next Day**\n",
    "\n",
    "**First Complete End to End Project**\n",
    "\n",
    "[Link](https://www.udemy.com/course/complete-mlops-bootcamp-with-10-end-to-end-ml-projects/learn/lecture/46209327#overview)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
