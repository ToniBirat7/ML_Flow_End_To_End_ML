{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f15a1316",
   "metadata": {},
   "source": [
    "# **Note 2 is Continued from Note 1**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bba8f3",
   "metadata": {},
   "source": [
    "## **Intro to ETL**\n",
    "\n",
    "ETL stands for Extract, Transform, Load. It is a process used in data warehousing and data integration to move data from various sources into a centralized repository, such as a data warehouse or a database. The ETL process consists of three main steps:\n",
    "\n",
    "1. **Extract**: This step involves retrieving data from various source systems, which can include databases, flat files, APIs, or other data sources. The goal is to gather all relevant data needed for analysis.\n",
    "\n",
    "2. **Transform**: In this step, the extracted data is cleaned, transformed, and prepared for analysis. This may involve filtering, aggregating, joining, or reshaping the data to ensure it is in the right format and structure for the target system.\n",
    "\n",
    "3. **Load**: The final step is to load the transformed data into the target system, such as a data warehouse or a database. This makes the data available for querying and analysis by business intelligence tools or other applications.\n",
    "\n",
    "## **ETL Process Overview**\n",
    "\n",
    "The ETL process can be visualized as a pipeline where data flows through each of the three stages. Hereâ€™s a high-level overview of the ETL process:\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[Extract] --> B[Transform]\n",
    "    B --> C[Load]\n",
    "    C --> D[Data Warehouse]\n",
    "    D --> E[Business Intelligence Tools]\n",
    "```\n",
    "\n",
    "## **Key Components of ETL**\n",
    "\n",
    "- **Data Sources**: These can be databases, flat files, APIs, or any other systems where data resides.\n",
    "\n",
    "- **ETL Tools**: Software applications that facilitate the ETL process, such as Apache Nifi, Talend, Informatica, or custom scripts.\n",
    "\n",
    "- **Data Warehouse**: A centralized repository where transformed data is stored for analysis and reporting.\n",
    "\n",
    "- **Data Quality**: Ensuring the accuracy, consistency, and reliability of the data throughout the ETL process.\n",
    "\n",
    "- **Scheduling and Automation**: ETL processes can be scheduled to run at specific intervals or triggered by events to ensure data is always up-to-date.\n",
    "\n",
    "## **Benefits of ETL**\n",
    "\n",
    "- **Centralized Data**: ETL allows organizations to consolidate data from multiple sources into a single repository, making it easier to access and analyze.\n",
    "\n",
    "- **Improved Data Quality**: The transformation step helps clean and standardize data, improving its quality for analysis.\n",
    "\n",
    "- **Enhanced Decision Making**: By providing a unified view of data, ETL supports better decision-making processes within organizations.\n",
    "\n",
    "- **Scalability**: ETL processes can be designed to handle large volumes of data, making them suitable for growing datasets.\n",
    "\n",
    "## **ETL vs. ELT**\n",
    "\n",
    "While ETL is a traditional approach, there is also a modern variant known as ELT (Extract, Load, Transform). In ELT, data is first extracted and loaded into the target system (like a data lake or data warehouse) before the transformation occurs. This approach leverages the processing power of modern databases to perform transformations after loading, allowing for more flexibility and scalability.\n",
    "\n",
    "## **Conclusion**\n",
    "\n",
    "ETL is a crucial process in data management that enables organizations to extract valuable insights from their data by transforming and loading it into a centralized repository. Understanding the ETL process is essential for anyone involved in data warehousing, business intelligence, or data integration projects. By implementing effective ETL practices, organizations can enhance their data quality, streamline their analytics processes, and make informed decisions based on accurate and timely information.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6df3fcd",
   "metadata": {},
   "source": [
    "## **End to End ETL Pipeline with Airflow**\n",
    "\n",
    "In this section, we will explore how to implement an end-to-end ETL pipeline using Apache Airflow, a powerful open-source tool for orchestrating complex workflows. Airflow allows you to define, schedule, and monitor ETL tasks efficiently.\n",
    "\n",
    "### **Problem Statement**\n",
    "\n",
    "We will create an ETL pipeline that extracts data from a public API, transforms it by cleaning and aggregating the data, and then loads it into a PostgreSQL database. The pipeline will be scheduled to run daily.\n",
    "\n",
    "We will use the NASA API to extract data about asteroids and their close approaches to Earth. The data will be transformed to calculate the average size of asteroids and then loaded into a PostgreSQL database for further analysis.\n",
    "\n",
    "We will dockerize the Airflow environment to ensure consistency and ease of deployment. The pipeline will consist of the following steps:\n",
    "\n",
    "1. **Extract**: Fetch data from the NASA API.\n",
    "2. **Transform**: Clean the data, calculate the average size of asteroids, and prepare it for loading.\n",
    "3. **Load**: Insert the transformed data into a PostgreSQL database.\n",
    "\n",
    "Both the Airflow and PostgreSQL services will be run in Docker containers, allowing for easy setup and management of the ETL pipeline. Here we will learn how the communication between the Airflow and PostgreSQL containers is established using Docker networking.\n",
    "\n",
    "We will also implement the airflow hooks to ensure that the pipeline runs smoothly and handles any errors that may occur during the ETL process.\n",
    "\n",
    "We will use different Airflow operators to perform the ETL tasks, including the `PythonOperator` for custom Python functions, the `PostgresOperator` for executing SQL commands, and the `DockerOperator` for running tasks in Docker containers and HTTP operator to make HTTP requests to the NASA API.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1327d8",
   "metadata": {},
   "source": [
    "## **Project Begins**\n",
    "\n",
    "Refer to `Airflow_ETL_Pipeline_Astro_Postgres`\n",
    "\n",
    "Note that folder name should never contain special characters like `(), ; -` etc.\n",
    "\n",
    "As by default both the `Airflow` and `Postgres` will be running in separate docker container. Therefore, we will need to establish a communication between these two.\n",
    "\n",
    "We create `docker-compose.yml` file that will create a `Postgres` image with db_name, uname, password and env_vars.\n",
    "\n",
    "### **Docker-Compose**\n",
    "\n",
    "```yml\n",
    "version: \"3\"\n",
    "services:\n",
    "  postgres:\n",
    "    image: postgres:13\n",
    "    container_name: postgres_db\n",
    "    environment:\n",
    "      POSTGRES_USER: birat\n",
    "      POSTGRES_PASSWORD: admin\n",
    "      POSTGRES_DB: postgres\n",
    "    ports:\n",
    "      - \"5432:5432\"\n",
    "    volumes:\n",
    "      - postgres_data:/var/lib/postgresql/data\n",
    "    networks:\n",
    "      - airflow_network\n",
    "\n",
    "networks:\n",
    "  airflow_network:\n",
    "    external: false\n",
    "```\n",
    "\n",
    "In the above `yml` file, we use the `postgres` image with all the environment variables.\n",
    "\n",
    "`Volume` tracks the data for consistency and `Network`\n",
    "\n",
    "We will need to have a common network so that the containers can talk.\n",
    "\n",
    "### **ETL DAG**\n",
    "\n",
    "```Python\n",
    "\n",
    "from airflow import DAG\n",
    "from airflow.providers.http.operators.http import SimpleHttpOperator\n",
    "from airflow.operators.python import PythonOperator\n",
    "from airflow.decorators import task\n",
    "from airflow.providers.postgres.hooks.postgres import PostgresHook\n",
    "import json\n",
    "from airflow.utils.dates import days_ago\n",
    "from httpx import post\n",
    "\n",
    "# Define the DAG\n",
    "\n",
    "default_args = {\n",
    "    'owner': 'airflow',\n",
    "    'depends_on_past': False,\n",
    "    'start_date': days_ago(1),\n",
    "    'retries': 1,\n",
    "}\n",
    "\n",
    "with DAG(\n",
    "    'etl_dag',\n",
    "    default_args=default_args,\n",
    "    description='A simple ETL DAG',\n",
    "    schedule='@daily',\n",
    ") as dag:\n",
    "\n",
    "  # Step 1: Create the table if it does not exist\n",
    "\n",
    "    @task\n",
    "    def create_table():\n",
    "        pg_hook = PostgresHook(\n",
    "            postgres_conn_id='postgres_default',\n",
    "        )\n",
    "\n",
    "        # SQL command to create the table for the API\n",
    "        create_table_sql = \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS apod_data (\n",
    "            id SERIAL PRIMARY KEY,\n",
    "            title VARCHAR(255),\n",
    "            explanation TEXT,\n",
    "            url TEXT,\n",
    "            date DATE,\n",
    "            media_type VARCHAR(50),\n",
    "        );\n",
    "        \"\"\"\n",
    "\n",
    "        # Execute the SQL command\n",
    "        pg_hook.run(create_table_sql)\n",
    "  # Step 2: Extract data from the API\n",
    "\n",
    "  # Step 3: Transform the data\n",
    "\n",
    "  # Step 4: Load the data into PostgreSQL\n",
    "\n",
    "  #\n",
    "\n",
    "```\n",
    "\n",
    "In the above code, we use `PostgresHook` to interact with the `Postgres`.\n",
    "\n",
    "`pg_hook.run(create_table_sql)` to execute the `SQL Query`\n",
    "\n",
    "**Setting Up the API**\n",
    "\n",
    "Below is the format of the API :\n",
    "\n",
    "```Json\n",
    "\n",
    "  \"copyright\": \"\\nIreneusz Nowak\\n\",\n",
    "  \"date\": \"2025-05-27\",\n",
    "  \"explanation\": \"Behold one of the most photogenic regions of the night sky, captured impressively.  Featured, the band of our Milky Way Galaxy runs diagonally along the bottom-left corner, while the colorful Rho Ophiuchi cloud complex is visible just right of center and the large red circular Zeta Ophiuchi Nebula appears near the top.  In general, red emanates from nebulas glowing in the light of excited hydrogen gas, while blue marks interstellar dust preferentially reflecting the light of bright young stars.  Thick dust usually appears dark brown.  Many iconic objects of the night sky appear, including (can you find them?) the bright star Antares, the globular star cluster M4, and the Blue Horsehead nebula. This wide field composite, taken over 17 hours, was captured from South Africa last June.    Explore Your Universe: Random APOD Generator\",\n",
    "  \"hdurl\": \"https://apod.nasa.gov/apod/image/2505/RhoZeta_Nowak_2560.jpg\",\n",
    "  \"media_type\": \"image\",\n",
    "  \"service_version\": \"v1\",\n",
    "  \"title\": \"Zeta and Rho Ophiuchi with Milky Way\",\n",
    "  \"url\": \"https://apod.nasa.gov/apod/image/2505/RhoZeta_Nowak_960.jpg\"\n",
    "}\n",
    "\n",
    "```\n",
    "\n",
    "```Py\n",
    "\n",
    "  # Step 2: Extract data from the API\n",
    "    # api_endpoint = 'https://api.nasa.gov/planetary/apod?api_key=2qhSecq7ZVI2TyDgfHGAhblGmrl2Q7ZZHdj1b6Ij'\n",
    "    extract_apod = SimpleHttpOperator(\n",
    "        task_id='extract_apod',\n",
    "        http_conn_id='apod_api', # Connection ID for the NASA API\n",
    "        endpoint='planetary/apod', # Endpoint for the Astronomy Picture of the Day\n",
    "        method='GET',\n",
    "        data={\"api_key\": \"{{ conn.nasa_api.extra_dejson.api_key }}\"}, # API key from the connection\n",
    "        response_filter=lambda response: response.json(), # Filter to get JSON response\n",
    "  )\n",
    "\n",
    "```\n",
    "\n",
    "We use `SimpleHttpOperator` to send HTTP request. In this hook we provide `http_conn_id = apod_api` we will make use of `Airflow` connection in the `UI`. Then `endpoint` as to hit the `API`, the base path will be available from Airflow. Then finally `data` we get the `api_key` which is also be received from the `Airflow` connection.\n",
    "\n",
    "Then finally we want the `response_filter` in `Json`.\n",
    "\n",
    "### **Transforming the Data**\n",
    "\n",
    "```Py\n",
    "\n",
    "    @task\n",
    "    def transform_data(response):\n",
    "        apod_data = {\n",
    "            \"title\": response.get(\"title\", \"\"),\n",
    "            \"explanation\": response.get(\"explanation\", \"\"),\n",
    "            \"url\": response.get(\"url\", \"\"),\n",
    "            \"date\": response.get(\"date\", \"\"),\n",
    "            \"media_type\": response.get(\"media_type\", \"\"),\n",
    "        }\n",
    "        return apod_data\n",
    "\n",
    "```\n",
    "\n",
    "### **Loading the Data**\n",
    "\n",
    "```Py\n",
    "\n",
    "    @task\n",
    "    def load_data_to_postgres(apo_data):\n",
    "        # Initialize PostgresHook\n",
    "\n",
    "        pg_hook = PostgresHook(postgres_conn_id=\"my_postgres_connection\")\n",
    "\n",
    "        # Define SQL Query\n",
    "        insert_sql = \"\"\"\n",
    "        INSERT INTO apod_data (title, explanation, url, date, media_type)\n",
    "        VALUES (%s, %s, %s, %s, %s);\n",
    "        \"\"\"\n",
    "\n",
    "        # Execute the insert query\n",
    "        pg_hook.run(\n",
    "            insert_sql,\n",
    "            parameters=(\n",
    "                apo_data[\"title\"],\n",
    "                apo_data[\"explanation\"],\n",
    "                apo_data[\"url\"],\n",
    "                apo_data[\"date\"],\n",
    "                apo_data[\"media_type\"],\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    # Define the task dependencies\n",
    "    # Extracting the APOD data and transforming it into a format suitable for PostgreSQL\n",
    "    create_table_task = create_table() >> extract_apod\n",
    "    extract_apod_task = extract_apod.output >> transform_data\n",
    "    # Transforming the data and loading it into PostgreSQL\n",
    "    transform_data_task = transform_data(extract_apod_task)\n",
    "    # Loading the transformed data into PostgreSQL\n",
    "    load_data_to_postgres(transform_data_task)\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f55bbb0",
   "metadata": {},
   "source": [
    "**Get Inside the Astro Container** : `astro dev bash`\n",
    "\n",
    "**Start Without Cache** : `astro dev restart --no-cache`\n",
    "\n",
    "### **Depricated Things in Airflow**\n",
    "\n",
    "`SimpleHttpOperator`, `Days_Before`\n",
    "\n",
    "### **Important Steps Before Running the Astro**\n",
    "\n",
    "As astro completely runs in the multiple Docker Container, it already by default installs all the required packages such as `Airflow`, `Postgres` and other. But, it does not install any other dependcies.\n",
    "\n",
    "Therefore, we will need to include the name of the package in the `requirement.txt` of `Astro` project.\n",
    "\n",
    "For example, I was getting a lots of error because I was installing the ` apache-airflow-providers-http` package in my environment but actually it needs to be installed inside the `container`.\n",
    "\n",
    "Therefore, always add the `dependencies` inside the `requirements.txt` file.\n",
    "\n",
    "Also, the `SimpleHttpOperator` is already depricated. We need to use `HttpOperator` which is provided by the third party providers.\n",
    "\n",
    "[Providers_Package_Ref](https://airflow.apache.org/docs/apache-airflow-providers/packages-ref.html#id49)\n",
    "\n",
    "The `username` and `password` in the `docker-compose` for the postgres should be `postgres` only. If we try to keep other names we will encounter error as `postgres` would be the default admin user while creating the container.\n",
    "\n",
    "### **Setting the Important Connection (API and DB)**\n",
    "\n",
    "Now, for the program to run properly we will need to setup the two important connection i.e. `Postgres` and other is `API`.\n",
    "\n",
    "**API**\n",
    "\n",
    "Go to the `Airflow` UI search for Connection.\n",
    "\n",
    "<img src='./Notes_Img/a1.png'>\n",
    "\n",
    "<img src='./Notes_Img/a2.png'>\n",
    "\n",
    "We need to fill these `infos`, later when the `DAG` is executed the info will be reterived from here. The name should be the same.\n",
    "\n",
    "**Postgres**\n",
    "\n",
    "Click add connection.\n",
    "\n",
    "Enter the `conn_id` used in the `DAG`. When the `Astro` starts the `Postgres` it will run the `Postgres` container with the details provided in the `docker-compose.yml` file. This file contains the `username` and `password`.\n",
    "\n",
    "For the `host` go the container look for `postgres` click then copy the name of the container and paste in the `Host`.\n",
    "\n",
    "<img src='./Notes_Img/a3.png'>\n",
    "\n",
    "Then excute the `DAG` from the `Airflow` UI.\n",
    "\n",
    "## **Possible Errors While Running DAG**\n",
    "\n",
    "Due to volume inconsistency the `Username` and `Password` won't be able to find by the `Postgres` container due to which we will have to remove the `Volume` from the container.\n",
    "\n",
    "```bash\n",
    "\n",
    "(mlflow_env) toni-birat@tonibirat:/media/toni-birat/New Volume/ML_Flow_Complete/Airflow_ETL_Pipeline_Astro_Postgres$ docker volume ls\n",
    "DRIVER    VOLUME NAME\n",
    "local     4d2e2ee6bd280a5cb2f26d998389a0fd0aef5270ce1d2814d9c22617e27b6d02\n",
    "local     59e773b558bc859c8819420ab883efbbd80ba0f820be1ef86ad10981f870f657\n",
    "local     airflow-etl-pipeline-astro-postgres_54d206_airflow_logs\n",
    "local     airflow-etl-pipeline-astro-postgres_54d206_postgres_data\n",
    "local     airflow-practice_d1986e_airflow_logs\n",
    "local     airflow-practice_d1986e_postgres_data\n",
    "local     mariadb_data\n",
    "(mlflow_env) toni-birat@tonibirat:/media/toni-birat/New Volume/ML_Flow_Complete/Airflow_ETL_Pipeline_Astro_Postgres$ docker volume rm airflow_etl_pipeline_astro_postgres_postgres_data\n",
    "Error response from daemon: get airflow_etl_pipeline_astro_postgres_postgres_data: no such volume\n",
    "(mlflow_env) toni-birat@tonibirat:/media/toni-birat/New Volume/ML_Flow_Complete/Airflow_ETL_Pipeline_Astro_Postgres$ docker volume rm airflow_etl_pipeline_astro_postgres_postgres_data\n",
    "Error response from daemon: get airflow_etl_pipeline_astro_postgres_postgres_data: no such volume\n",
    "(mlflow_env) toni-birat@tonibirat:/media/toni-birat/New Volume/ML_Flow_Complete/Airflow_ETL_Pipeline_Astro_Postgres$ docker volume rm airflow-etl-pipeline-astro-postgres_54d206_airflow_logs\n",
    "airflow-etl-pipeline-astro-postgres_54d206_airflow_logs\n",
    "(mlflow_env) toni-birat@tonibirat:/media/toni-birat/New Volume/ML_Flow_Complete/Airflow_ETL_Pipeline_Astro_Postgres$ docker volume rm airflow-etl-pipeline-astro-postgres_54d206_postgres_data\n",
    "airflow-etl-pipeline-astro-postgres_54d206_postgres_data\n",
    "(mlflow_env) toni-birat@tonibirat:/media/toni-birat/New Volume/ML_Flow_Complete/Airflow_ETL_Pipeline_Astro_Postgres$ docker volume rm airflow-practice_d1986e_postgres_data\n",
    "airflow-practice_d1986e_postgres_data\n",
    "(mlflow_env) toni-birat@tonibirat:/media/toni-birat/New Volume/ML_Flow_Complete/Airflow_ETL_Pipeline_Astro_Postgres$\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1452e1",
   "metadata": {},
   "source": [
    "Once all the DAGs are completed we've successfully implement or automated the complete `ETL` pipeline project.\n",
    "\n",
    "As we've `Postgres` container we can't directly viewe the tables and our rows. For that we will need to install `dbeaver community`\n",
    "\n",
    "<img src='./Notes_Img/a4.png'>\n",
    "\n",
    "**DBeaver**\n",
    "\n",
    "<img src='./Notes_Img/a5.png'>\n",
    "\n",
    "Go to the Database, look for `apod_data`\n",
    "\n",
    "<img src='./Notes_Img/a6.png'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bea15f5",
   "metadata": {},
   "source": [
    "### **Advantage of Providing the Connection Variables from Airflow UI**\n",
    "\n",
    "We can pass any creds, remote host id, api keys.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f04dfc",
   "metadata": {},
   "source": [
    "## **Deploying the Astro Project in the Astro Cloud and AWS**\n",
    "\n",
    "[Video_Link](https://www.udemy.com/course/complete-mlops-bootcamp-with-10-end-to-end-ml-projects/learn/lecture/46199315#overview)\n",
    "\n",
    "**Astro.io**\n",
    "\n",
    "We will host the airflow application in the Astro Cloud. Login, Create Account.\n",
    "\n",
    "Create the organization. Name the project. \n",
    "\n",
    "**AWS Database**\n",
    "\n",
    "Create a posgres\n",
    "\n",
    "<hr>\n",
    "\n",
    "We will use `Astro CLI` for the deployment.\n",
    "\n",
    "`astro login`\n",
    "\n",
    "`astro deploy`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b4dbf7",
   "metadata": {},
   "source": [
    "## **Tomorrow**\n",
    "\n",
    "Fix the DAG problem. Retry with\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
